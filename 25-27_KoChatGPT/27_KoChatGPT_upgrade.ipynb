{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOV269GNVM0qAgzHpJE1I0T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9e387616168540fcaa8d89e143af67b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a91fb755228d492482561977976872ed","IPY_MODEL_40fc0a00737144caaf881b447189451c","IPY_MODEL_594fa69745be4c96b354d1a4ed6972ed"],"layout":"IPY_MODEL_7a7ecd7c2e1a466da0b10f5af5f9ac50"}},"a91fb755228d492482561977976872ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_764c49899dd8445e90c8f8872f96cece","placeholder":"​","style":"IPY_MODEL_321a2de4b0e0406ca991db5dfc812843","value":"config.json: 100%"}},"40fc0a00737144caaf881b447189451c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40b2e0eed3e24b4391215b1239663455","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5155992674cd47e48698761623661b20","value":1000}},"594fa69745be4c96b354d1a4ed6972ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd663fe223e041faa51d026de9f70bed","placeholder":"​","style":"IPY_MODEL_c7efe99984f24eb389050a173fa91e97","value":" 1.00k/1.00k [00:00&lt;00:00, 54.2kB/s]"}},"7a7ecd7c2e1a466da0b10f5af5f9ac50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"764c49899dd8445e90c8f8872f96cece":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"321a2de4b0e0406ca991db5dfc812843":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40b2e0eed3e24b4391215b1239663455":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5155992674cd47e48698761623661b20":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd663fe223e041faa51d026de9f70bed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7efe99984f24eb389050a173fa91e97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab882d722f574d7ea95440d3daa348b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e83eeadef4bf45e3bf1e022a755993f0","IPY_MODEL_61b6ed83c827498f8ae1ac76e6fabf8c","IPY_MODEL_9b5a7e80d71243f4925911c65b4eda26"],"layout":"IPY_MODEL_aee5389351c04d7985e48e4d8c621518"}},"e83eeadef4bf45e3bf1e022a755993f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23da481836db4139ad7371d82a6adb1d","placeholder":"​","style":"IPY_MODEL_03fc527c48ef4847afb9f71ff0bd72b8","value":"tokenizer.json: 100%"}},"61b6ed83c827498f8ae1ac76e6fabf8c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c4cfa1b363e445ebd6520b0a11cc912","max":2825034,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29dd39ba0e624c4eb1ca6f4cb18d3f93","value":2825034}},"9b5a7e80d71243f4925911c65b4eda26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc775888e58144c3bb83502ae5e68870","placeholder":"​","style":"IPY_MODEL_fa6bcd191bbe443684cf37d503e39afe","value":" 2.83M/2.83M [00:00&lt;00:00, 8.25MB/s]"}},"aee5389351c04d7985e48e4d8c621518":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23da481836db4139ad7371d82a6adb1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03fc527c48ef4847afb9f71ff0bd72b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c4cfa1b363e445ebd6520b0a11cc912":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29dd39ba0e624c4eb1ca6f4cb18d3f93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc775888e58144c3bb83502ae5e68870":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa6bcd191bbe443684cf37d503e39afe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f782d69b51204463aec99b274b7db168":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_086b53aca1ce475b836ba00114e0433f","IPY_MODEL_df5d73cb27fc4cafad756b7b2f500807","IPY_MODEL_2738e6364c694b52ae0b9e87a050f471"],"layout":"IPY_MODEL_84322a0560f54185b0c08d396d525cad"}},"086b53aca1ce475b836ba00114e0433f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23d94cc0cff4468da04cff68f80c2c94","placeholder":"​","style":"IPY_MODEL_56500103b17f4921bc3528a7b2d3a160","value":"pytorch_model.bin: 100%"}},"df5d73cb27fc4cafad756b7b2f500807":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3c5d2cfe19d4f438a8036a22924fc00","max":513302779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d77afbc82aed4379b34305b5faaada99","value":513302779}},"2738e6364c694b52ae0b9e87a050f471":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_730047210ae44f7b8cde68026b85a186","placeholder":"​","style":"IPY_MODEL_d1d5465cb7f547139f9c932d69b7b11b","value":" 513M/513M [00:02&lt;00:00, 222MB/s]"}},"84322a0560f54185b0c08d396d525cad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23d94cc0cff4468da04cff68f80c2c94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56500103b17f4921bc3528a7b2d3a160":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3c5d2cfe19d4f438a8036a22924fc00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d77afbc82aed4379b34305b5faaada99":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"730047210ae44f7b8cde68026b85a186":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1d5465cb7f547139f9c932d69b7b11b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#프로젝트: KoChatGPT 업그레이드\n","\n","### 목표\n","아래 제시된 또는 그밖의 방법으로 커스텀 ChatGPT를 개발한다.\n","\n","- 기존 KoChatGPT 모델에 사용한 데이터셋 정제\n","\n","- SFT와 RM 모델에 쓰이는 사람 피드백(Human Feedback) 데이터셋을 다른 benchmark 데이터셋으로 대체\n","\n","- 디코딩 성능 개선을 위한 하이퍼파라미터 조정\n","\n","- 정량 메트릭 설정하여 모델 선택\n","\n","- 지도 조정(Instruction Tuning) 및 프롬프팅 기법 적용\n","\n","- 더 큰 기반 모델(foundation model) 사용\n","\n","- 저순위 적응(LoRA, Low Rank Adaptaion), 지도 조정, 보상 순위(reward ranking) 알고리즘 도입\n","\n","### 주의사항\n","\n","기존 소스코드는 빠른 실습을 위해 원본 코드를 일부 수정하였으므로, 모델을 커스터마이징 과정에서 필요한 경우 \"colossalai_ChatGPT_230319\" 폴더 내의 원본 스크립트들을 참고할 것\n","\n","### 실행 예시\n","#### 데이터셋 정제\n","- `data_kochatgpt` 디렉토리 내의 `kochatgpt_1_SFT.jsonl`, `kochatgpt_1_RM.jsonl`,`kochatgpt_1_PPO.jsonl` 파일을 분석(EDA)하여 정제\n","- 정제 후 데이터 증강 기법 적용하여 줄어든 데이터 크기를 유지 또는 증량\n","\n","#### 새 데이터셋 추가\n","- 공개댓글을 활용해 순위(ranking) 데이터셋 구축한 앤스로픽(Anthropic) RLHF 구현을 참고하여, 동일 프롬프트에 대한 여러 반응(댓글)을 크롤링하여 차등적 보상을 적용하는 지시/랭킹 데이터셋 구축\n","\n","-  KorQuad 등 한국어 이해 벤치마크를 활용한 고품질 데이터셋을 확보해 KoGPT-2의 저품질 데이터셋과 페어링\n","\n","- 다양한 데이터 증강 시도하여 모델 재학습\n","\n","#### 기반모델 변경\n","- 허깅페이스가 제공하는 경량화/최적화 라이브러리 적용하여 튜닝 (예: skt/ko-gpt-trinity 모델)\n","- 메모리 초과(OOM) 문제 예방을 위해 적절한 훈련 인자 조정이 필요\n","- 기반 모델 교체 후 generator 함수를 수정하여 추론 결과 제시\n","\n","\n","### 평가\n","- 새로운 모델 학습 후 BLEU, ROUGE등 정량 지표 활용하여 추론 성능 비교"],"metadata":{"id":"97TvyzgXi94e"}},{"cell_type":"markdown","source":["# 환경 설정"],"metadata":{"id":"WDy7vgKjstHK"}},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# 구글 드라이브 마운트\n","drive.mount('/content/drive')\n","\n","# 코랩 환경에서 실행을 위핸 패키지 업데이트 (실행 후 세션 재시작)\n","chatgpt_package_path = '/content/drive/MyDrive/Data/KoChatGPT/colossalai_ChatGPT_230319'\n","\n","!pip install {chatgpt_package_path} datasets loralib accelerate\n","!pip install colossalai==0.2.8\n","\n","os.kill(os.getpid(), 9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2golbCkzUM1k","outputId":"c8f29492-2fbd-4502-cddd-b168b3e56b87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Processing ./drive/MyDrive/Data/KoChatGPT/colossalai_ChatGPT_230319\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting datasets\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting loralib\n","  Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n","Collecting accelerate\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers>=4.20.1 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.35.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.66.1)\n","Collecting colossalai>=0.2.4 (from chatgpt==0.1.0)\n","  Downloading colossalai-0.3.4.tar.gz (985 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m985.8/985.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.1.0+cu118)\n","Collecting langchain (from chatgpt==0.1.0)\n","  Downloading langchain-0.0.348-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n","Collecting pre-commit (from colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading pre_commit-3.5.0-py2.py3-none-any.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.7.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.7)\n","Collecting fabric (from colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading fabric-3.2.2-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting contexttimer (from colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ninja (from colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops (from colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.10.13)\n","Collecting ray (from colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading ray-2.8.1-cp310-cp310-manylinux2014_x86_64.whl (62.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece (from colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (2.0.3)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.20.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.15.0)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (2.0.23)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain->chatgpt==0.1.0)\n","  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain->chatgpt==0.1.0)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting langchain-core<0.1,>=0.0.12 (from langchain->chatgpt==0.1.0)\n","  Downloading langchain_core-0.0.12-py3-none-any.whl (181 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain->chatgpt==0.1.0)\n","  Downloading langsmith-0.0.69-py3-none-any.whl (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.2.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain->chatgpt==0.1.0)\n","  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain->chatgpt==0.1.0)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain->chatgpt==0.1.0)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.1,>=0.0.12->langchain->chatgpt==0.1.0) (3.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->chatgpt==0.1.0) (3.0.1)\n","Collecting invoke>=2.0 (from fabric->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading invoke-2.2.0-py3-none-any.whl (160 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting paramiko>=2.4 (from fabric->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading paramiko-3.3.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting decorator>=5 (from fabric->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Collecting deprecated>=1.2 (from fabric->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google->colossalai>=0.2.4->chatgpt==0.1.0) (4.11.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->chatgpt==0.1.0) (2.1.3)\n","Collecting cfgv>=2.0.0 (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n","Collecting identify>=1.0.0 (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading identify-2.5.33-py2.py3-none-any.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n","Collecting virtualenv>=20.10.0 (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.2.4->chatgpt==0.1.0) (4.19.2)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.2.4->chatgpt==0.1.0) (1.0.7)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.16.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->chatgpt==0.1.0) (1.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.12->langchain->chatgpt==0.1.0) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.12->langchain->chatgpt==0.1.0) (1.2.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (67.7.2)\n","Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading bcrypt-4.1.1-cp37-abi3-manylinux_2_28_x86_64.whl (699 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m699.4/699.4 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (41.0.7)\n","Collecting pynacl>=1.5 (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->chatgpt==0.1.0)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0)\n","  Downloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google->colossalai>=0.2.4->chatgpt==0.1.0) (2.5)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.2.4->chatgpt==0.1.0) (2023.11.2)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.2.4->chatgpt==0.1.0) (0.31.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.2.4->chatgpt==0.1.0) (0.13.2)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n","Building wheels for collected packages: chatgpt, colossalai, contexttimer\n","  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46644 sha256=c7c7d20fa289861d1575398fa40e82d7c9f407e0f8be0902cf1d392a5605f4c1\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-nppjeoi8/wheels/54/8a/d7/7bdc915fb81e458a563a14885c32dd2b80e12de2bce9df4535\n","  Building wheel for colossalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for colossalai: filename=colossalai-0.3.4-py3-none-any.whl size=1243173 sha256=b98db95b2ff6ea52932db6e9308280345ad6b83ddb1096189492379ba2c30a2f\n","  Stored in directory: /root/.cache/pip/wheels/8f/38/95/f314c7707791bfe1fb4ab978e403dfc445dde535b3a3415697\n","  Building wheel for contexttimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5804 sha256=54ee5ebac0e319287f36c5cf72ae71f8762adb7664236d7d1a6ba18f501105c8\n","  Stored in directory: /root/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4\n","Successfully built chatgpt colossalai contexttimer\n","Installing collected packages: sentencepiece, ninja, distlib, contexttimer, virtualenv, pyarrow-hotfix, nodeenv, mypy-extensions, marshmallow, loralib, jsonpointer, invoke, identify, einops, dill, deprecated, decorator, cfgv, bcrypt, typing-inspect, pynacl, pre-commit, multiprocess, langsmith, jsonpatch, paramiko, langchain-core, dataclasses-json, accelerate, ray, langchain, fabric, datasets, colossalai, chatgpt\n","  Attempting uninstall: decorator\n","    Found existing installation: decorator 4.4.2\n","    Uninstalling decorator-4.4.2:\n","      Successfully uninstalled decorator-4.4.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.25.0 bcrypt-4.1.1 cfgv-3.4.0 chatgpt-0.1.0 colossalai-0.3.4 contexttimer-0.3.3 dataclasses-json-0.6.3 datasets-2.15.0 decorator-5.1.1 deprecated-1.2.14 dill-0.3.7 distlib-0.3.7 einops-0.7.0 fabric-3.2.2 identify-2.5.33 invoke-2.2.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.348 langchain-core-0.0.12 langsmith-0.0.69 loralib-0.1.2 marshmallow-3.20.1 multiprocess-0.70.15 mypy-extensions-1.0.0 ninja-1.11.1.1 nodeenv-1.8.0 paramiko-3.3.1 pre-commit-3.5.0 pyarrow-hotfix-0.6 pynacl-1.5.0 ray-2.8.1 sentencepiece-0.1.99 typing-inspect-0.9.0 virtualenv-20.25.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["decorator"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting colossalai==0.2.8\n","  Downloading colossalai-0.2.8.tar.gz (728 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.1/728.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.8) (1.23.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.8) (4.66.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.8) (5.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.8) (23.2)\n","Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.8) (3.5.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.8) (13.7.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.8) (8.1.7)\n","Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.8) (3.2.2)\n","Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.8) (0.3.3)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.8) (1.11.1.1)\n","Collecting torch<2.0,>=1.11 (from colossalai==0.2.8)\n","  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.0,>=1.11->colossalai==0.2.8) (4.5.0)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.0,>=1.11->colossalai==0.2.8)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.0,>=1.11->colossalai==0.2.8)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.0,>=1.11->colossalai==0.2.8)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.0,>=1.11->colossalai==0.2.8)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0,>=1.11->colossalai==0.2.8) (67.7.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0,>=1.11->colossalai==0.2.8) (0.42.0)\n","Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.8) (2.2.0)\n","Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.8) (3.3.1)\n","Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.8) (5.1.1)\n","Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.8) (1.2.14)\n","Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.8) (3.4.0)\n","Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.8) (2.5.33)\n","Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.8) (1.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.8) (6.0.1)\n","Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.8) (20.25.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.8) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.8) (2.16.1)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai==0.2.8) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai==0.2.8) (0.1.2)\n","Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.8) (4.1.1)\n","Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.8) (41.0.7)\n","Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.8) (1.5.0)\n","Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.8) (0.3.7)\n","Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.8) (3.13.1)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.8) (4.0.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.8) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.8) (2.21)\n","Building wheels for collected packages: colossalai\n","  Building wheel for colossalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for colossalai: filename=colossalai-0.2.8-py3-none-any.whl size=930922 sha256=1da1ec8a108777586fc0457651196d465e4542366ea9904ee547e0901a693467\n","  Stored in directory: /root/.cache/pip/wheels/91/4d/77/9e915ab39a58f922cb9c4db126e8a082b56131e81339d64fcb\n","Successfully built colossalai\n","Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, colossalai\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu118\n","    Uninstalling torch-2.1.0+cu118:\n","      Successfully uninstalled torch-2.1.0+cu118\n","  Attempting uninstall: colossalai\n","    Found existing installation: colossalai 0.3.4\n","    Uninstalling colossalai-0.3.4:\n","      Successfully uninstalled colossalai-0.3.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed colossalai-0.2.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n"]}]},{"cell_type":"code","source":["## 버전 확인\n","\n","import torch\n","\n","print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n","print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n","!pip list | grep transformers # transformers 4.28.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMxNLCHavHYM","executionInfo":{"status":"ok","timestamp":1702015598656,"user_tz":-540,"elapsed":2722,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"6b1f577a-a814-4966-94d5-400abf1cf84d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version:1.13.1+cu117\n","Cuda version: 11.7\n","transformers                     4.35.2\n"]}]},{"cell_type":"markdown","source":["# 모델 및 토크나이저 불러오기"],"metadata":{"id":"_lGPyVXBx3Va"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_name = \"skt/kogpt2-base-v2\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["9e387616168540fcaa8d89e143af67b4","a91fb755228d492482561977976872ed","40fc0a00737144caaf881b447189451c","594fa69745be4c96b354d1a4ed6972ed","7a7ecd7c2e1a466da0b10f5af5f9ac50","764c49899dd8445e90c8f8872f96cece","321a2de4b0e0406ca991db5dfc812843","40b2e0eed3e24b4391215b1239663455","5155992674cd47e48698761623661b20","dd663fe223e041faa51d026de9f70bed","c7efe99984f24eb389050a173fa91e97","ab882d722f574d7ea95440d3daa348b5","e83eeadef4bf45e3bf1e022a755993f0","61b6ed83c827498f8ae1ac76e6fabf8c","9b5a7e80d71243f4925911c65b4eda26","aee5389351c04d7985e48e4d8c621518","23da481836db4139ad7371d82a6adb1d","03fc527c48ef4847afb9f71ff0bd72b8","9c4cfa1b363e445ebd6520b0a11cc912","29dd39ba0e624c4eb1ca6f4cb18d3f93","cc775888e58144c3bb83502ae5e68870","fa6bcd191bbe443684cf37d503e39afe","f782d69b51204463aec99b274b7db168","086b53aca1ce475b836ba00114e0433f","df5d73cb27fc4cafad756b7b2f500807","2738e6364c694b52ae0b9e87a050f471","84322a0560f54185b0c08d396d525cad","23d94cc0cff4468da04cff68f80c2c94","56500103b17f4921bc3528a7b2d3a160","e3c5d2cfe19d4f438a8036a22924fc00","d77afbc82aed4379b34305b5faaada99","730047210ae44f7b8cde68026b85a186","d1d5465cb7f547139f9c932d69b7b11b"]},"id":"bAjfPsUyvRFR","executionInfo":{"status":"ok","timestamp":1702015614252,"user_tz":-540,"elapsed":15599,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"0039585a-35d4-455f-e587-9128ec742dc8"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e387616168540fcaa8d89e143af67b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab882d722f574d7ea95440d3daa348b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f782d69b51204463aec99b274b7db168"}},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy\n","\n","# 토크나이저 작동 확인\n","input_txt = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n","\n","input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()\n","tokens = tokenizer(input_txt).tokens()\n","\n","df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179},"id":"GdiTvhMnvf0S","executionInfo":{"status":"ok","timestamp":1702015615309,"user_tz":-540,"elapsed":1062,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"6b76c80e-1aab-4982-b273-bac7dca0478a"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                   0     1     2      3      4     5     6     7     8     9   \\\n","kogpt-2_tokens    ▁바람     도   ▁없는   ▁공중에    ▁수직     의    ▁파    문을    ▁내    이며   \n","Input_IDs       10891  7235  9712  49207  14438  8143  9203  9941  9094  9639   \n","\n","                ...     13     14     15    16     17     18    19    20  \\\n","kogpt-2_tokens  ...  ▁떨어지는    ▁오동     잎은    ▁누     구의    ▁발자     취    ▁입   \n","Input_IDs       ...  21215  34769  19985  9669  10139  21626  8408  9241   \n","\n","                   21   22  \n","kogpt-2_tokens     니까    .  \n","Input_IDs       23775  389  \n","\n","[2 rows x 23 columns]"],"text/html":["\n","  <div id=\"df-538b66dc-8829-43fd-b16d-dfc677466650\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>kogpt-2_tokens</th>\n","      <td>▁바람</td>\n","      <td>도</td>\n","      <td>▁없는</td>\n","      <td>▁공중에</td>\n","      <td>▁수직</td>\n","      <td>의</td>\n","      <td>▁파</td>\n","      <td>문을</td>\n","      <td>▁내</td>\n","      <td>이며</td>\n","      <td>...</td>\n","      <td>▁떨어지는</td>\n","      <td>▁오동</td>\n","      <td>잎은</td>\n","      <td>▁누</td>\n","      <td>구의</td>\n","      <td>▁발자</td>\n","      <td>취</td>\n","      <td>▁입</td>\n","      <td>니까</td>\n","      <td>.</td>\n","    </tr>\n","    <tr>\n","      <th>Input_IDs</th>\n","      <td>10891</td>\n","      <td>7235</td>\n","      <td>9712</td>\n","      <td>49207</td>\n","      <td>14438</td>\n","      <td>8143</td>\n","      <td>9203</td>\n","      <td>9941</td>\n","      <td>9094</td>\n","      <td>9639</td>\n","      <td>...</td>\n","      <td>21215</td>\n","      <td>34769</td>\n","      <td>19985</td>\n","      <td>9669</td>\n","      <td>10139</td>\n","      <td>21626</td>\n","      <td>8408</td>\n","      <td>9241</td>\n","      <td>23775</td>\n","      <td>389</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 23 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-538b66dc-8829-43fd-b16d-dfc677466650')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-538b66dc-8829-43fd-b16d-dfc677466650 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-538b66dc-8829-43fd-b16d-dfc677466650');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f94b2537-215a-496e-9a0c-d0d76a134684\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f94b2537-215a-496e-9a0c-d0d76a134684')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f94b2537-215a-496e-9a0c-d0d76a134684 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# 디코딩 모델 작동 확인 (탐욕적 탐색)\n","input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","output_greedy = model.generate(input_ids, max_length=128,\n","                               do_sample=False) # 예상 토큰 인덱스 벡터\n","\n","print(tokenizer.decode(output_greedy[0])) # 토크나이저의 디코더로 출력"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhJJNEH-wDt6","executionInfo":{"status":"ok","timestamp":1702015623298,"user_tz":-540,"elapsed":7999,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"8c57d6f3-99eb-427f-df38-08fd4cd6a095"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n","\"그렇다면 그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리요?\"\n","\"그건 무슨 소리\n"]}]},{"cell_type":"code","source":["# 모델 작동 확인 (빔 서치; n-gram 페널티 부여)\n","output_beam = model.generate(input_ids, max_length=128, num_beams=10,\n","                             no_repeat_ngram_size=2, do_sample=False)\n","print(tokenizer.decode(output_beam[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"urq69E4kxQFO","executionInfo":{"status":"ok","timestamp":1702015628389,"user_tz":-540,"elapsed":5098,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"17aca65d-998a-4d3b-cb67-c069f6b241d6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n","\"그렇지 않습니다.\"\n","\"어떻게 된 일입니까?\"\n","그녀는 고개를 갸웃거렸다.\n","\"아니, 그게 무슨 말씀이신지 모르겠습니다만.\"\n","\"무슨 말씀인지 알 수가 없군요.\"\n","아무런 대답도 하지 않은 채 그녀는 고개를 끄덕였다.\n","\"그래, 알았어.\"\n","그녀의 눈에서 눈물이 주르륵 흘러내렸다.\n","그녀가 다시 입을 열었다.\n","\"정말 죄송합니다, 고마워요, 고맙습니다\"\n","\"\n"]}]},{"cell_type":"code","source":["# 모델 작동 확인 (빔 서치; n-gram 페널티 부여, top_k 샘플링 기법 추가)\n","output_beam = model.generate(input_ids, max_length=128, num_beams=7, no_repeat_ngram_size=2,\n","                             do_sample=True, temperature=2.0, top_k=50)\n","print(tokenizer.decode(output_beam[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCeHQx1vxett","executionInfo":{"status":"ok","timestamp":1702015630987,"user_tz":-540,"elapsed":2606,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"f6786cd1-5e3c-4404-c49c-6ffa35a5aa07"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.</d> 올해 수능에서 국어 1등급 커트라인은 국어 A·B등급의 평균 점수인 5.06점이고, 수학 가형은 국어가 5.47점, 수학 나형은 5.27점으로 나타났다.\n","1등급과 2등급을 합치면 국어 A, B등급은 국어, 수학B, 영어, 사회/과학, 과학탐구가 당락을 가를 것으로 보인다.\n","지난해 수준과 비슷하거나 약간 상승한 수치다.\n","평가원 관계자는 \"수능은 지난 6월 모의평가와 비교해 다소 어렵게 출\n"]}]},{"cell_type":"code","source":["# 모델 작동 확인 (빔 서치; n-gram 페널티 부여, top_p 샘플링 기법 추가)\n","output_beam = model.generate(input_ids, max_length=128, num_beams=7, no_repeat_ngram_size=2,\n","                             do_sample=True, top_p=0.90)\n","print(tokenizer.decode(output_beam[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YM8V2Mhxswm","executionInfo":{"status":"ok","timestamp":1702015634639,"user_tz":-540,"elapsed":3660,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"f1b91886-5793-4050-8069-7ebb24f3c35c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n","\"그렇지. 그게 무슨 소리냐?\"\n","그녀는 고개를 갸웃거렸다.\n","\"아니, 그건 그렇고.\"\n","\"어떻게 된 일입니까.\"\n","그녀가 고개를 끄덕였다.\n","\"무슨 말씀을 하시는 겁니까, 당신. 당신이 그걸 어떻게 알겠습니까.\"\n","그러자 그녀는 고개를 저었다.\n","\"당신은 그 말을 믿지 않습니다. 당신은 당신에게 그런 말을 한 적이 없습니다.\"\n","이윽고 그녀는 한숨을 내쉬었다.\n","그녀의 눈에는 눈물이\n"]}]},{"cell_type":"markdown","source":["# 데이터셋 불러오기"],"metadata":{"id":"zVofMTCgx47B"}},{"cell_type":"code","source":["# 지도 미조정(Supervised Fine-Tuning) 훈련 데이터\n","import json\n","from os.path import join as path_join\n","\n","repo_path = '/content/drive/MyDrive/Data/KoChatGPT/'\n","sft_jsonl = path_join(repo_path, 'data_kochatgpt/kochatgpt_1_SFT.jsonl')\n","with open(sft_jsonl, \"r\", encoding='utf-8-sig') as json_file:\n","  list_data_dict = json.load(json_file)\n","\n","print(f'데이터셋 길이: {len(list_data_dict)}')\n","list_data_dict[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvnfiftQxxO_","executionInfo":{"status":"ok","timestamp":1702015635318,"user_tz":-540,"elapsed":685,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"99927c96-6a19-4109-a4da-3ee89c023d20"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["데이터셋 길이: 12000\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'prompt': '불고기용 고기 한우에요?',\n","  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n","  'tokens': 193},\n"," {'prompt': '쓰던 앱이 유료로 전환됐어',\n","  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n","  'tokens': 288},\n"," {'prompt': '여친이랑 다툼',\n","  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n","  'tokens': 153}]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# 보상 모델(Reward Model) 훈련 데이터\n","rm_jsonl = path_join(repo_path, 'data_kochatgpt/kochatgpt_2_RM.jsonl')\n","with open(rm_jsonl, \"r\", encoding='utf-8-sig') as json_file:\n","    list_data_dict = json.load(json_file)\n","\n","print(len(list_data_dict))\n","list_data_dict[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wurNhvN81Ocj","executionInfo":{"status":"ok","timestamp":1702015636800,"user_tz":-540,"elapsed":1489,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"8706d18a-816c-40b2-eb55-acd24b1136de"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["10220\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?',\n","  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n","  'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.',\n","  'completion_2': '라이언에게 말했다.',\n","  'ranking': [2, 1, 0]},\n"," {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?',\n","  'completion_0': '개포주공아파트는 다섯 단지로 이루어져 있습니다.',\n","  'completion_1': '이날 목송에서 구글상위노',\n","  'completion_2': '개포주공아파트는 총 27개 단지로 이루어져 있습니다.',\n","  'ranking': [2, 0, 1]},\n"," {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?',\n","  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n","  'completion_1': '이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가 그 발언을 문제삼았는지에 따라 답이 다를 수 있습니다.\\\\n\\\\n만약 김영삼 대통령이 후보 시절에 지역표심을 겨냥한 발언을 했다는 가정하에, 그 발언을 문제삼은 후보가 누구였는지를 대답하자면, 그 답은 이화선 당시 민주당 대통령 후보가 될 것입니다. 1992년 총선 때, 김영삼 대선후보는 \"집값이 오른 노량진역 부근의 부동산 가격은 세월호 폭침 후 \\\\\\'강남 도시재생\\\\\\' 일환으로 상승했다\"는 발언을 했습니다. 하지만 이화선 후보는 이 발언을 \"전국적으로 경제적 발전이 이루어지지 않은 지방민의 마음을 멀리해지려는 무례한 발언\"이라고 비판하며 문제삼았습니다.\\\\n\\\\n하지만, 이 질문을 답변하는 데 있어서 보다 명확한 정보가 있으면 답변을 보완할 수 있습니다.',\n","  'completion_2': '김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 추구하고 있는 민주주의 광범위하게 확립과 보수의 사상을 이어가는 데 있어 지역경제 발전과 공공서비스 신속 개선을 위해 합리적인 국가 정책에 따르는 방향성을 제시하고 있습니다.',\n","  'ranking': [1, 2, 0]}]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# 인접향 최적화(Proximal Policy Optimization) 훈련 데이터\n","ppo_jsonl =  path_join(repo_path, 'data_kochatgpt/kochatgpt_3_PPO.jsonl')\n","with open(ppo_jsonl, \"r\", encoding='utf-8-sig') as json_file:\n","    list_data_dict = json.load(json_file)\n","\n","print(len(list_data_dict))\n","list_data_dict[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ua8W-ARuykL_","executionInfo":{"status":"ok","timestamp":1702015637532,"user_tz":-540,"elapsed":740,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"2cb2eb57-8f94-414d-8bb6-7bc704d5ccf4"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["12000\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n"," {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n"," {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# 지도 미조정(Supervised Fine-Tuning, SFT)"],"metadata":{"id":"iq8G7FNy1bWH"}},{"cell_type":"markdown","source":["## 모델 및 토큰 로드"],"metadata":{"id":"oRbMh6t4RwYq"}},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n","tokenizer = AutoTokenizer.from_pretrained(\n","    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n","    padding_side=\"right\",\n","    model_max_length=512)"],"metadata":{"id":"ehtezi2h1McJ","executionInfo":{"status":"ok","timestamp":1702015646121,"user_tz":-540,"elapsed":8596,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["print(tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JPbF7S7hBJeW","executionInfo":{"status":"ok","timestamp":1702015646121,"user_tz":-540,"elapsed":22,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"fc352fc3-3b09-444c-c553-f3a5fd78eeda"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"<usr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t4: AddedToken(\"<sys>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t5: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t6: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t7: AddedToken(\"<d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t8: AddedToken(\"</d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t9: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t10: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t11: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t12: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t13: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t14: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t15: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t16: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t17: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t18: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t19: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t20: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t21: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t22: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t23: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t24: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t25: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t26: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t27: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t28: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t29: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t30: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t31: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t32: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t33: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t34: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t35: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t36: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t37: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t38: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t39: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t40: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t41: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t42: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t43: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t44: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t45: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t46: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t47: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t48: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t49: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t51: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t52: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t53: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t54: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t55: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t56: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t57: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t58: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t59: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t60: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t61: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t62: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t63: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t64: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t65: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t66: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t67: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t68: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t69: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t70: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t71: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t72: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t73: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t74: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t75: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t76: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t77: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t78: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t79: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t80: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t81: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t82: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t83: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t84: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t85: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t86: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t87: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t88: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t89: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t90: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t91: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t92: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t93: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t94: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t95: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t96: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t97: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t98: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t99: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t104: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t105: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t106: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t107: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t108: AddedToken(\"<unused99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t109: AddedToken(\":-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t110: AddedToken(\":)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t111: AddedToken(\"-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t112: AddedToken(\"(-:\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t113: AddedToken(\"(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t114: AddedToken(\"(:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t115: AddedToken(\"-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t116: AddedToken(\"8-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t117: AddedToken(\"'-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t118: AddedToken(\":-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t119: AddedToken(\":-*\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t120: AddedToken(\":-/\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t121: AddedToken(\":->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t122: AddedToken(\":-@\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t123: AddedToken(\":-d\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t124: AddedToken(\":-V\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t125: AddedToken(\":-X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t126: AddedToken(\":-\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t127: AddedToken(\":-]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t128: AddedToken(\";-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t129: AddedToken(\">;->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t130: AddedToken(\";^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t131: AddedToken(\"%-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t132: AddedToken(\"):-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t133: AddedToken(\"3:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t134: AddedToken(\":-&\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t135: AddedToken(\"8:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t136: AddedToken(\":-)8<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t137: AddedToken(\":-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t138: AddedToken(\":-6\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t139: AddedToken(\"+:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t140: AddedToken(\"O:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t141: AddedToken(\":-<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t142: AddedToken(\":-?\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t143: AddedToken(\":-E\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t144: AddedToken(\":-Q\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t145: AddedToken(\":-}X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t146: AddedToken(\":-[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t147: AddedToken(\":-a\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t148: AddedToken(\":-{\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t149: AddedToken(\":-{}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t150: AddedToken(\":^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151: AddedToken(\"<:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t152: AddedToken(\":=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t153: AddedToken(\">:->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t154: AddedToken(\">:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t155: AddedToken(\"@:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t156: AddedToken(\"@:-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t157: AddedToken(\"C=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t158: AddedToken(\"X:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t159: AddedToken(\"[:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t160: AddedToken(\"[:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t161: AddedToken(\"{:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t162: AddedToken(\"l^o\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t163: AddedToken(\"}:^#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t164: AddedToken(\":-(=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t165: AddedToken(\"O-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t166: AddedToken(\":-3\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t167: AddedToken(\":=\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t168: AddedToken(\":-\"\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t169: AddedToken(\"P-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t170: AddedToken(\"?-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t171: AddedToken(\"d:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t172: AddedToken(\":8)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t173: AddedToken(\":-7\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t174: AddedToken(\"):-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t175: AddedToken(\":/\\)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t176: AddedToken(\"8(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t177: AddedToken(\"([(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t178: AddedToken(\":-(*)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t179: AddedToken(\"&-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t180: AddedToken(\":-e\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t181: AddedToken(\":(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t182: AddedToken(\":,(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t183: AddedToken(\":-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t184: AddedToken(\":-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t185: AddedToken(\":-S\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t186: AddedToken(\":-C\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t187: AddedToken(\":-r\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t188: AddedToken(\":-t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t189: AddedToken(\":-W\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t190: AddedToken(\"X-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t191: AddedToken(\"l-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t192: AddedToken(\"l:-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t193: AddedToken(\"$-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t194: AddedToken(\":-!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t195: AddedToken(\":----}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t196: AddedToken(\"=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t197: AddedToken(\"=:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t198: AddedToken(\"3:[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t199: AddedToken(\"8<:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t200: AddedToken(\":#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t201: AddedToken(\"8-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t202: AddedToken(\"B-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t203: AddedToken(\"8-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t204: AddedToken(\"|-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t205: AddedToken(\"H-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t206: AddedToken(\"]-I\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t207: AddedToken(\"V^J\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t208: AddedToken(\"+-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t209: AddedToken(\"~:-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t210: AddedToken(\"`'\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t211: AddedToken(\"L-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t212: AddedToken(\"BI\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t213: AddedToken(\"O|\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t214: AddedToken(\"^^\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t215: AddedToken(\"ㅜㅜ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t216: AddedToken(\"ㅠㅠ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t217: AddedToken(\"ㅡㅡ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t218: AddedToken(\"😠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t219: AddedToken(\"👿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t220: AddedToken(\"😧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t221: AddedToken(\"😰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t222: AddedToken(\"😲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t223: AddedToken(\"😁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t224: AddedToken(\"🐻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t225: AddedToken(\"🐱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t226: AddedToken(\"😹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t227: AddedToken(\"😼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t228: AddedToken(\"🤡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t229: AddedToken(\"🥶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t230: AddedToken(\"😖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t231: AddedToken(\"😕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t232: AddedToken(\"🐮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t233: AddedToken(\"🤠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t234: AddedToken(\"😿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t235: AddedToken(\"😢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t236: AddedToken(\"😞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t237: AddedToken(\"😵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t238: AddedToken(\"🐶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t239: AddedToken(\"😓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t240: AddedToken(\"🐲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t241: AddedToken(\"🤤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t242: AddedToken(\"😑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t243: AddedToken(\"😘\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t244: AddedToken(\"😋\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t245: AddedToken(\"😱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t246: AddedToken(\"🤮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t247: AddedToken(\"🤭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t248: AddedToken(\"🤕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t249: AddedToken(\"😷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t250: AddedToken(\"🧐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t251: AddedToken(\"😮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t252: AddedToken(\"🤨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t253: AddedToken(\"🙄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t254: AddedToken(\"😤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t255: AddedToken(\"🤬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t256: AddedToken(\"😂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t257: AddedToken(\"🤒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t258: AddedToken(\"😛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t259: AddedToken(\"😶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t260: AddedToken(\"😨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t261: AddedToken(\"🌛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t262: AddedToken(\"😳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t263: AddedToken(\"🦊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t264: AddedToken(\"🐸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t265: AddedToken(\"☹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t266: AddedToken(\"☹️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t267: AddedToken(\"😦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t268: AddedToken(\"🌝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t269: AddedToken(\"😬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t270: AddedToken(\"😺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t271: AddedToken(\"😸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t272: AddedToken(\"😀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t273: AddedToken(\"😃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t274: AddedToken(\"😄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t275: AddedToken(\"😅\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t276: AddedToken(\"😆\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t277: AddedToken(\"🐹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t278: AddedToken(\"🐴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t279: AddedToken(\"🥵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t280: AddedToken(\"🤗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t281: AddedToken(\"😯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t282: AddedToken(\"😽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t283: AddedToken(\"😗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t284: AddedToken(\"😚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t285: AddedToken(\"😙\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t286: AddedToken(\"🌜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t287: AddedToken(\"🦁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t288: AddedToken(\"😭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t289: AddedToken(\"🤥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t290: AddedToken(\"🤦🏿‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t291: AddedToken(\"🤦🏻‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t292: AddedToken(\"🤦🏾‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t293: AddedToken(\"🤦🏼‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t294: AddedToken(\"🤦🏽‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t295: AddedToken(\"🤦‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t296: AddedToken(\"🤦🏿‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t297: AddedToken(\"🤦🏻‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t298: AddedToken(\"🤦🏾‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t299: AddedToken(\"🤦🏼‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t300: AddedToken(\"🤦🏽‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t301: AddedToken(\"🤦‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t302: AddedToken(\"🤑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t303: AddedToken(\"🐵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t304: AddedToken(\"🐭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t305: AddedToken(\"🤢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t306: AddedToken(\"🤓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t307: AddedToken(\"😐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t308: AddedToken(\"🌚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t309: AddedToken(\"🐼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t310: AddedToken(\"🥳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t311: AddedToken(\"😔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t312: AddedToken(\"😣\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t313: AddedToken(\"🤦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t314: AddedToken(\"🤦🏿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t315: AddedToken(\"🤦🏻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t316: AddedToken(\"🤦🏾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t317: AddedToken(\"🤦🏼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t318: AddedToken(\"🤦🏽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t319: AddedToken(\"🐷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t320: AddedToken(\"🥺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t321: AddedToken(\"😾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t322: AddedToken(\"😡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t323: AddedToken(\"🐰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t324: AddedToken(\"😌\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t325: AddedToken(\"🤖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t326: AddedToken(\"😥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t327: AddedToken(\"🤫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t328: AddedToken(\"😴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t329: AddedToken(\"😪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t330: AddedToken(\"🙁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t331: AddedToken(\"🙂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t332: AddedToken(\"😻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t333: AddedToken(\"☺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t334: AddedToken(\"☺️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t335: AddedToken(\"🥰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t336: AddedToken(\"😇\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t337: AddedToken(\"😍\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t338: AddedToken(\"😈\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t339: AddedToken(\"😊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t340: AddedToken(\"😎\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t341: AddedToken(\"😏\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t342: AddedToken(\"🤧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t343: AddedToken(\"😝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t344: AddedToken(\"🌞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t345: AddedToken(\"🤔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t346: AddedToken(\"🐯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t347: AddedToken(\"😫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t348: AddedToken(\"😒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t349: AddedToken(\"🦄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t350: AddedToken(\"🙃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t351: AddedToken(\"🙀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t352: AddedToken(\"😩\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t353: AddedToken(\"🌬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t354: AddedToken(\"🌬️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t355: AddedToken(\"😉\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t356: AddedToken(\"😜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t357: AddedToken(\"🐺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t358: AddedToken(\"🤦🏿‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t359: AddedToken(\"🤦🏻‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t360: AddedToken(\"🤦🏾‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t361: AddedToken(\"🤦🏼‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t362: AddedToken(\"🤦🏽‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t363: AddedToken(\"🤦‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t364: AddedToken(\"🤦🏿‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t365: AddedToken(\"🤦🏻‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t366: AddedToken(\"🤦🏾‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t367: AddedToken(\"🤦🏼‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t368: AddedToken(\"🤦🏽‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t369: AddedToken(\"🤦‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t370: AddedToken(\"🥴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t371: AddedToken(\"😟\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t372: AddedToken(\"🥱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t373: AddedToken(\"🤪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t374: AddedToken(\"🤐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n"]}]},{"cell_type":"markdown","source":["## SFT 데이터셋 클래스, 콜레이터 클래스 정의"],"metadata":{"id":"W26LuNsuR0Rd"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import transformers, logging, copy\n","from typing import Optional, Dict, Sequence\n","\n","# 프롬프트 형식 정의(전역 변수)\n","PROMPT_DICT = {\n","    \"prompt_input\": (\n","        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n","    )\n","}\n","\n","class SFT_dataset(Dataset):\n","    # 클래스 초기화 함수\n","    def __init__(self, data_path_1_SFT: str,\n","                 tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n","        super(SFT_dataset, self).__init__()\n","        logging.warning(\"Loading data...\")\n","\n","        # JSON 파일에서 데이터 로드\n","        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n","            list_data_dict = json.load(json_file)\n","\n","        # 소스 데이터 생성\n","        sources = [\n","            PROMPT_DICT[\"prompt_input\"].format_map(example) for example in list_data_dict]\n","\n","        # 타겟 데이터 생성\n","        targets = [\n","            f\"{example['completion']}{tokenizer.eos_token}\" for example in list_data_dict]\n","\n","        # 소스와 타겟 결합\n","        examples = [\n","            f\"{source}{target}\" for source, target in zip(sources, targets)\n","        ]\n","\n","        # 토큰화 수행\n","        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n","        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n","\n","        # input_ids와 labels 준비\n","        input_ids = examples_tokenized[\"input_ids\"]\n","        labels = copy.deepcopy(input_ids)\n","        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n","            label[:source_len] = -100  # 소스 부분은 학습에서 제외\n","\n","        data_dict = dict(input_ids=input_ids, labels=labels)\n","\n","        # 클래스 인스턴스 변수 설정\n","        self.input_ids = data_dict[\"input_ids\"]\n","        self.labels = data_dict[\"labels\"]\n","        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n","\n","    # 토큰화 함수\n","    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n","        tokenized_list = [\n","            tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                padding=\"longest\",\n","                max_length=tokenizer.model_max_length,\n","                truncation=True,\n","            )\n","            for text in strings\n","        ]\n","        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n","        input_ids_lens = labels_lens = [\n","            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n","        ]\n","        return dict(\n","            input_ids=input_ids,\n","            labels=labels,\n","            input_ids_lens=input_ids_lens,\n","            labels_lens=labels_lens,\n","        )\n","\n","    # 데이터셋 길이 반환 함수\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    # 특정 인덱스의 데이터 반환 함수\n","    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n","        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n"],"metadata":{"id":"jCScqkweGtyt","executionInfo":{"status":"ok","timestamp":1702015646121,"user_tz":-540,"elapsed":16,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from typing import Sequence, Dict\n","from torch.nn.utils.rnn import pad_sequence\n","from dataclasses import dataclass\n","\n","@dataclass\n","class DataCollatorForSupervisedDataset(object):\n","    # 토크나이저 정의 (transformers 라이브러리에서 제공하는 PreTrainedTokenizer)\n","    tokenizer: transformers.PreTrainedTokenizer\n","\n","    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n","        # 각 인스턴스에서 'input_ids'와 'labels'를 추출하여 튜플로 구성\n","        input_ids, labels = zip(*[(instance['input_ids'], instance['labels']) for instance in instances])\n","\n","        # 'input_ids'와 'labels'를 패딩하여 동일한 길이로 맞춤\n","        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n","        labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # -100은 손실 계산 시 무시됨\n","\n","        # attention_mask 생성: 패딩된 부분은 0, 그 외는 1\n","        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n","\n","        # 결과 딕셔너리 반환\n","        return {\n","            \"input_ids\": input_ids,\n","            \"labels\": labels,\n","            \"attention_mask\": attention_mask,\n","        }"],"metadata":{"id":"0DLr0-vzMydf","executionInfo":{"status":"ok","timestamp":1702015646121,"user_tz":-540,"elapsed":16,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# 훈련셋과 콜레이터 인스턴스 생성\n","train_dataset = SFT_dataset(sft_jsonl, tokenizer=tokenizer)\n","data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n","\n","print('input : %s'%train_dataset.input_ids[0])\n","print('output: %s'%train_dataset.labels[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfBEnIW1NN_-","executionInfo":{"status":"ok","timestamp":1702015663431,"user_tz":-540,"elapsed":17326,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"dfae12d9-b93c-4f38-9da3-517400e13d74"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Loading data...\n","WARNING:root:Loading data done!!: 12000\n"]},{"output_type":"stream","name":"stdout","text":["input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n","         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n","          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n","         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n","         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n","        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n","        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n","         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n","        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n","        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n","         8711,  9046, 12450,  9117,  7377, 12521,     1])\n","output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n","         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n","        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n","        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n","         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n","        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n","        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n","         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"]}]},{"cell_type":"code","source":["# 토큰 디코더\n","def decode_dataset_example(train_dataset, tokenizer, index=0):\n","    input_ids_example = train_dataset.input_ids[index]\n","    labels_example = train_dataset.labels[index]\n","\n","    # -100 값을 제외하고 디코딩\n","    decoded_input = tokenizer.decode(input_ids_example, skip_special_tokens=True)\n","    decoded_labels = tokenizer.decode([t for t in labels_example if t != -100], skip_special_tokens=True)\n","\n","    return decoded_input, decoded_labels\n","\n","# 사용 예시\n","decoded_input, decoded_labels = decode_dataset_example(train_dataset, tokenizer, 1)\n","print(\"Decoded Input:\\n\", decoded_input)\n","print(\"\")\n","print(\"Decoded Labels:\\n\", decoded_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_gw1j9FRG-y","executionInfo":{"status":"ok","timestamp":1702015663431,"user_tz":-540,"elapsed":12,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"9c5a32b9-c789-4862-fefb-afa4fe333a17"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoded Input:\n"," ### Instruction(명령어):\n","쓰던 앱이 유료로 전환됐어\n","\n","### Response(응답):'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\n","\n","Decoded Labels:\n"," '어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\n"]}]},{"cell_type":"markdown","source":["## 훈련 클래스 정의"],"metadata":{"id":"_vJ_aASLRsRI"}},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","import os\n","\n","training_args = TrainingArguments(\n","    output_dir = path_join(repo_path, \"test\"),\n","    overwrite_output_dir=True,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    warmup_steps=5,\n","    prediction_loss_only=True,\n","    fp16 = True\n",")\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset\n",")"],"metadata":{"id":"s0Km8zEmSwNp","executionInfo":{"status":"ok","timestamp":1702016417329,"user_tz":-540,"elapsed":896,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# 훈련 진행\n","trainer.train()\n","model_path_SFT = path_join(repo_path, \"output_1_SFT\")\n","model.save_pretrained(model_path_SFT)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"gSp3pLVQTSO1","executionInfo":{"status":"ok","timestamp":1702016830016,"user_tz":-540,"elapsed":411993,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"882e0d4b-1431-4ee9-fcca-59afa5783506"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/1500 06:49, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>2.867800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.789500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>2.694600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"markdown","source":["## 문장 생성"],"metadata":{"id":"peMuiQuHVTdR"}},{"cell_type":"code","source":["from transformers import pipeline\n","generator = pipeline('text-generation', model=model_path_SFT, tokenizer=tokenizer)\n","\n","generation_args = dict(\n","    num_beams=4,\n","    repetition_penalty=2.0,\n","    no_repeat_ngram_size=4,\n","    eos_token_id=375, # \\n\n","    max_new_tokens=64,\n","    do_sample=True,\n","    top_k=50,\n","    early_stopping=True\n",")\n","\n","list_prompt = ['불고기용 고기 한우에요?',\n","               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n","               '시카고 오헤어 국제공항은 어디에 있어?',\n","               '오늘 미세먼지 어때?']\n","\n","list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n","\n","list_result = generator(list_prompt, **generation_args)\n","for prompt, result in zip(list_prompt, list_result):\n","    print()\n","    print((result[0]['generated_text']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPW_TZIOVRIy","executionInfo":{"status":"ok","timestamp":1702016877894,"user_tz":-540,"elapsed":47887,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"2125eeb9-cf0e-474d-8c39-1407f761335c"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","### Instruction(명령어):\n","불고기용 고기 한우에요?\n","\n","### Response(응답):'저는 인공지능 어시스턴트이기 때문에 정확한 답변을 드리기 어렵습니다. 하지만 일반적으로 불고기용 고기를 판매하는 가게나 음식점에 따라 다를 수 있으니, 해당 가게나 음식점의 공식 홈페이지나 전화번호를 확인해보시는 것을 추천드립니다. 감사합니다. you please provide m\n","\n","### Instruction(명령어):\n","리처드 닉슨이 43대 부통령직을 수행한 년도는?\n","\n","### Response(응답):'리처드 닉슨은 41대 부통령직을 수행했습니다. J. W. Brown (Richard Johnson) J. W. B. Brown(Richard D. Nixon) J. W.B. Brown(Translation of the Capability) J. E. Brow\n","\n","### Instruction(명령어):\n","시카고 오헤어 국제공항은 어디에 있어?\n","\n","### Response(응답):'시카고 오 헤어 국제공항은 미국 캘리포니아주 로스앤젤레스에 위치해 있습니다. Jefferson of Canada Operator 국제공항 (California Pacific Airportation Corporation) jefferson to Korean Capital Admin\n","\n","### Instruction(명령어):\n","오늘 미세먼지 어때?\n","\n","### Response(응답):'저는 인공지능 챗봇이기 때문에 미세먼지 문제에 대한 정보를 가지고 있지 않습니다. 하지만 미세먼지 문제가 심각하다는 것은 맞습니다. 미세먼지 농도가 높은 날에는 야외활동을 자제하는 것이 좋습니다. 또한 외출 시 마스크를 착용하거나 손세정제를 사용하는 것도 미세먼지를 줄이는 데 도움이\n"]}]},{"cell_type":"markdown","source":["# 보상 모델(RM, Reward Model)"],"metadata":{"id":"Jcsasq4xVQdq"}},{"cell_type":"code","source":["# 메모리 확보를 위해 캐시 삭제\n","torch.cuda.empty_cache()"],"metadata":{"id":"fh7uOEw8WKOm","executionInfo":{"status":"ok","timestamp":1702016877895,"user_tz":-540,"elapsed":12,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"3RGJr5TM3duC"}},{"cell_type":"markdown","source":["## 사용자 정의 보상 모델 클래스"],"metadata":{"id":"Fms9fdyIZ2Ja"}},{"cell_type":"code","source":["from chatgpt.models.base import RewardModel\n","from typing import Optional\n","from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n","from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n","import torch.nn as nn\n","from chatgpt.trainer.strategies import NaiveStrategy\n","\n","class GPTRM_custom(RewardModel):\n","\n","    # 초기화 함수\n","    def __init__(self,\n","                 pretrained: Optional[str] = None,\n","                 config: Optional[GPT2Config] = None,\n","                 checkpoint: bool = False,\n","                 lora_rank: int = 0,\n","                 lora_train_bias: str = 'none',\n","                 tokenizer=None) -> None:\n","        # 사전 학습된 모델을 로드하는 경우\n","        if pretrained is not None:\n","            model = GPT2Model.from_pretrained(pretrained)\n","            model.resize_token_embeddings(len(tokenizer))\n","        # 설정 객체를 사용하여 모델을 초기화하는 경우\n","        elif config is not None:\n","            model = GPT2Model(config)\n","        # 기본 설정으로 모델을 초기화하는 경우\n","        else:\n","            model = GPT2Model(GPT2Config())\n","        # 그래디언트 체크포인팅 활성화\n","        if checkpoint:\n","            model.gradient_checkpointing_enable()\n","\n","        # 값(value) 헤드 추가\n","        value_head = nn.Linear(model.config.n_embd, 1)\n","        super().__init__(model, value_head, lora_rank, lora_train_bias)\n","\n","        # 사전 학습된 모델 정보 저장\n","        if pretrained is not None:\n","            self.model = model\n","            self.pretrained = pretrained\n","\n","    # 모델 저장 함수\n","    def save_pretrained(self, dir):\n","        if self.pretrained is not None:\n","            self.model.save_pretrained(dir)\n","\n","# GPT-2 모델과 토크나이저 로드\n","model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n","tokenizer = AutoTokenizer.from_pretrained(\n","    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n","    padding_side=\"right\",\n","    model_max_length=512,\n",")\n","\n","# 사용자 정의 모델 초기화 및 CUDA로 이동\n","with NaiveStrategy().model_init_context():\n","        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()\n"],"metadata":{"id":"Z1c6PKp0Zzgf","executionInfo":{"status":"ok","timestamp":1702016887984,"user_tz":-540,"elapsed":10100,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# 모델 및 토크나이저 정의\n","model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n","tokenizer = AutoTokenizer.from_pretrained(\n","    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n","    padding_side=\"right\",\n","    model_max_length=512,\n",")\n","\n","with NaiveStrategy().model_init_context():\n","        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"],"metadata":{"id":"SKsB10fZclIr","executionInfo":{"status":"ok","timestamp":1702016892188,"user_tz":-540,"elapsed":4214,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# 훈련용 순위 데이터셋(ranking dataset) 생성\n","\n","rm_jsonl = path_join(repo_path, 'data_kochatgpt/kochatgpt_2_RM.jsonl')\n","with open(rm_jsonl, \"r\", encoding='utf-8-sig') as json_file:\n","  list_data_dict = json.load(json_file)\n","\n","total_data_ranking2chosen = []\n","for tmp in list_data_dict:\n","    one_data_ranking2chosen = []\n","\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][0] < tmp['ranking'][1]:\n","        data['chosen'] = tmp['completion_0']\n","        data['rejected'] = tmp['completion_1']\n","    else:\n","        data['chosen'] = tmp['completion_1']\n","        data['rejected'] = tmp['completion_0']\n","    one_data_ranking2chosen.append(data)\n","\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][0] < tmp['ranking'][2]:\n","        data['chosen'] = tmp['completion_0']\n","        data['rejected'] = tmp['completion_2']\n","    else:\n","        data['chosen'] = tmp['completion_2']\n","        data['rejected'] = tmp['completion_0']\n","    one_data_ranking2chosen.append(data)\n","\n","    data = {}\n","    data['prompt'] = tmp['prompt']\n","    if tmp['ranking'][1] < tmp['ranking'][2]:\n","        data['chosen'] = tmp['completion_1']\n","        data['rejected'] = tmp['completion_2']\n","    else:\n","        data['chosen'] = tmp['completion_2']\n","        data['rejected'] = tmp['completion_1']\n","    one_data_ranking2chosen.append(data)\n","\n","\n","\n","    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n","\n","print('before data num: %d'%(len(list_data_dict)))\n","print('after  data num: %d'%(len(total_data_ranking2chosen)))\n","print('data example: \\n%s'%total_data_ranking2chosen[45])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"si6_PYjVco8T","executionInfo":{"status":"ok","timestamp":1702016892188,"user_tz":-540,"elapsed":13,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"81369966-30c2-4d07-b158-43a294d9dd76"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["before data num: 10220\n","after  data num: 30660\n","data example: \n","{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"]}]},{"cell_type":"code","source":["# 순위 데이터셋 섞은 후 훈련셋 생성\n","import random\n","from chatgpt.dataset import RewardDataset\n","\n","random.seed(230319)\n","random.shuffle(total_data_ranking2chosen)\n","\n","# 신속한 테스트를 위해 데이터 일부만 학습\n","train_data = total_data_ranking2chosen[:1000]\n","eval_data = total_data_ranking2chosen[1000:1200]\n","\n","print(len(train_data))\n","print(len(eval_data))\n","\n","train_dataset = RewardDataset(train_data, tokenizer, 512)\n","eval_dataset = RewardDataset(eval_data, tokenizer, 512)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bOM3M2luc58m","executionInfo":{"status":"ok","timestamp":1702016893592,"user_tz":-540,"elapsed":1416,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"cc57f825-2cb8-48b9-fa90-66ca225d95cb"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["1000\n","200\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:01<00:00, 892.56it/s]\n","100%|██████████| 200/200 [00:00<00:00, 743.26it/s]\n"]}]},{"cell_type":"code","source":["# 데이터셋 내용 확인\n","idx = 1\n","print('#'*70)\n","print('## prompt ##')\n","print(train_data[idx]['prompt'])\n","print('#'*70)\n","print('## chosen ##')\n","print(train_data[idx]['chosen'])\n","print('#'*70)\n","print('## rejected ##')\n","print(train_data[idx]['rejected'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cttkI17FdFBU","executionInfo":{"status":"ok","timestamp":1702016893592,"user_tz":-540,"elapsed":5,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"fdd5a31f-5995-404a-e7d5-56b5c7cccc2d"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["######################################################################\n","## prompt ##\n","흑고래의 무게는 어느 정도야\n","######################################################################\n","## chosen ##\n","흑고래의 평균 몸무게는 약 25~40톤 정도이지만, 최대 몸무게는 50톤 이상에 이를 수 있습니다.\n","######################################################################\n","## rejected ##\n","흑고래의 무게는 매우 다양하게 달라집니다. 약 200kg에서 10톤까지 달라질 수 있습니다.\n"]}]},{"cell_type":"code","source":["from chatgpt.trainer import RewardModelTrainer\n","from torch.optim import Adam\n","\n","# 순위 모델 학습\n","trainer = RewardModelTrainer(model=model,\n","                             strategy=NaiveStrategy(),\n","                             optim=Adam(model.parameters(), lr=5e-5),\n","                             train_dataset=train_dataset,\n","                             eval_dataset=eval_dataset,\n","                             batch_size=4,\n","                             max_epochs=1)\n","\n","trainer.fit(use_lora=0)\n","\n","model_path_RM = path_join(repo_path, \"output_2_RM\")\n","model.save_pretrained(model_path_RM)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hKy4I_h2dJrq","executionInfo":{"status":"ok","timestamp":1702017126399,"user_tz":-540,"elapsed":232811,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"2adcb018-9ea3-4461-c64a-39b77189da55"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n","Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n","Train step of epoch 0:   0%|          | 1/250 [00:00<03:42,  1.12it/s]\u001b[A\n","Train step of epoch 0:   0%|          | 1/250 [00:00<03:42,  1.12it/s, loss=0.596]\u001b[A\n","Train step of epoch 0:   1%|          | 2/250 [00:01<03:36,  1.15it/s, loss=0.596]\u001b[A\n","Train step of epoch 0:   1%|          | 2/250 [00:01<03:36,  1.15it/s, loss=0.417]\u001b[A\n","Train step of epoch 0:   1%|          | 3/250 [00:02<03:34,  1.15it/s, loss=0.417]\u001b[A\n","Train step of epoch 0:   1%|          | 3/250 [00:02<03:34,  1.15it/s, loss=0.54] \u001b[A\n","Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:32,  1.16it/s, loss=0.54]\u001b[A\n","Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:32,  1.16it/s, loss=0.647]\u001b[A\n","Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:31,  1.16it/s, loss=0.647]\u001b[A\n","Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:31,  1.16it/s, loss=0.534]\u001b[A\n","Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:29,  1.16it/s, loss=0.534]\u001b[A\n","Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:29,  1.16it/s, loss=0.595]\u001b[A\n","Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:29,  1.16it/s, loss=0.595]\u001b[A\n","Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:29,  1.16it/s, loss=0.357]\u001b[A\n","Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:28,  1.16it/s, loss=0.357]\u001b[A\n","Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:28,  1.16it/s, loss=2.56] \u001b[A\n","Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:27,  1.16it/s, loss=2.56]\u001b[A\n","Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:27,  1.16it/s, loss=0.393]\u001b[A\n","Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:26,  1.16it/s, loss=0.393]\u001b[A\n","Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:26,  1.16it/s, loss=0.592]\u001b[A\n","Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:26,  1.16it/s, loss=0.592]\u001b[A\n","Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:26,  1.16it/s, loss=0.693]\u001b[A\n","Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:25,  1.16it/s, loss=0.693]\u001b[A\n","Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:25,  1.16it/s, loss=0.508]\u001b[A\n","Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:25,  1.16it/s, loss=0.508]\u001b[A\n","Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:25,  1.16it/s, loss=0.211]\u001b[A\n","Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:24,  1.16it/s, loss=0.211]\u001b[A\n","Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:24,  1.16it/s, loss=0.959]\u001b[A\n","Train step of epoch 0:   6%|▌         | 15/250 [00:12<03:23,  1.15it/s, loss=0.959]\u001b[A\n","Train step of epoch 0:   6%|▌         | 15/250 [00:12<03:23,  1.15it/s, loss=0.588]\u001b[A\n","Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:22,  1.15it/s, loss=0.588]\u001b[A\n","Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:22,  1.15it/s, loss=0.755]\u001b[A\n","Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:22,  1.15it/s, loss=0.755]\u001b[A\n","Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:22,  1.15it/s, loss=0.714]\u001b[A\n","Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:21,  1.15it/s, loss=0.714]\u001b[A\n","Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:21,  1.15it/s, loss=0.575]\u001b[A\n","Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:21,  1.15it/s, loss=0.575]\u001b[A\n","Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:21,  1.15it/s, loss=0.655]\u001b[A\n","Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:20,  1.15it/s, loss=0.655]\u001b[A\n","Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:20,  1.15it/s, loss=0.607]\u001b[A\n","Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:19,  1.15it/s, loss=0.607]\u001b[A\n","Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:19,  1.15it/s, loss=0.724]\u001b[A\n","Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:19,  1.14it/s, loss=0.724]\u001b[A\n","Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:19,  1.14it/s, loss=0.789]\u001b[A\n","Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:19,  1.14it/s, loss=0.789]\u001b[A\n","Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:19,  1.14it/s, loss=0.867]\u001b[A\n","Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:18,  1.14it/s, loss=0.867]\u001b[A\n","Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:18,  1.14it/s, loss=0.662]\u001b[A\n","Train step of epoch 0:  10%|█         | 25/250 [00:21<03:17,  1.14it/s, loss=0.662]\u001b[A\n","Train step of epoch 0:  10%|█         | 25/250 [00:21<03:17,  1.14it/s, loss=0.531]\u001b[A\n","Train step of epoch 0:  10%|█         | 26/250 [00:22<03:16,  1.14it/s, loss=0.531]\u001b[A\n","Train step of epoch 0:  10%|█         | 26/250 [00:22<03:16,  1.14it/s, loss=0.651]\u001b[A\n","Train step of epoch 0:  11%|█         | 27/250 [00:23<03:16,  1.13it/s, loss=0.651]\u001b[A\n","Train step of epoch 0:  11%|█         | 27/250 [00:23<03:16,  1.13it/s, loss=0.663]\u001b[A\n","Train step of epoch 0:  11%|█         | 28/250 [00:24<03:15,  1.13it/s, loss=0.663]\u001b[A\n","Train step of epoch 0:  11%|█         | 28/250 [00:24<03:15,  1.13it/s, loss=0.538]\u001b[A\n","Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:15,  1.13it/s, loss=0.538]\u001b[A\n","Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:15,  1.13it/s, loss=0.51] \u001b[A\n","Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:14,  1.13it/s, loss=0.51]\u001b[A\n","Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:14,  1.13it/s, loss=0.54]\u001b[A\n","Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:13,  1.13it/s, loss=0.54]\u001b[A\n","Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:13,  1.13it/s, loss=0.458]\u001b[A\n","Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:12,  1.13it/s, loss=0.458]\u001b[A\n","Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:12,  1.13it/s, loss=0.813]\u001b[A\n","Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:11,  1.13it/s, loss=0.813]\u001b[A\n","Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:11,  1.13it/s, loss=0.506]\u001b[A\n","Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:10,  1.13it/s, loss=0.506]\u001b[A\n","Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:10,  1.13it/s, loss=0.504]\u001b[A\n","Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:09,  1.13it/s, loss=0.504]\u001b[A\n","Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:09,  1.13it/s, loss=0.714]\u001b[A\n","Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:08,  1.13it/s, loss=0.714]\u001b[A\n","Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:08,  1.13it/s, loss=1.13] \u001b[A\n","Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:07,  1.13it/s, loss=1.13]\u001b[A\n","Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:07,  1.13it/s, loss=0.798]\u001b[A\n","Train step of epoch 0:  15%|█▌        | 38/250 [00:33<03:06,  1.14it/s, loss=0.798]\u001b[A\n","Train step of epoch 0:  15%|█▌        | 38/250 [00:33<03:06,  1.14it/s, loss=0.665]\u001b[A\n","Train step of epoch 0:  16%|█▌        | 39/250 [00:34<03:05,  1.14it/s, loss=0.665]\u001b[A\n","Train step of epoch 0:  16%|█▌        | 39/250 [00:34<03:05,  1.14it/s, loss=0.723]\u001b[A\n","Train step of epoch 0:  16%|█▌        | 40/250 [00:34<03:04,  1.14it/s, loss=0.723]\u001b[A\n","Train step of epoch 0:  16%|█▌        | 40/250 [00:34<03:04,  1.14it/s, loss=0.454]\u001b[A\n","Train step of epoch 0:  16%|█▋        | 41/250 [00:35<03:03,  1.14it/s, loss=0.454]\u001b[A\n","Train step of epoch 0:  16%|█▋        | 41/250 [00:35<03:03,  1.14it/s, loss=0.38] \u001b[A\n","Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:02,  1.14it/s, loss=0.38]\u001b[A\n","Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:02,  1.14it/s, loss=0.622]\u001b[A\n","Train step of epoch 0:  17%|█▋        | 43/250 [00:37<03:00,  1.14it/s, loss=0.622]\u001b[A\n","Train step of epoch 0:  17%|█▋        | 43/250 [00:37<03:00,  1.14it/s, loss=0.667]\u001b[A\n","Train step of epoch 0:  18%|█▊        | 44/250 [00:38<02:59,  1.15it/s, loss=0.667]\u001b[A\n","Train step of epoch 0:  18%|█▊        | 44/250 [00:38<02:59,  1.15it/s, loss=0.9]  \u001b[A\n","Train step of epoch 0:  18%|█▊        | 45/250 [00:39<02:58,  1.15it/s, loss=0.9]\u001b[A\n","Train step of epoch 0:  18%|█▊        | 45/250 [00:39<02:58,  1.15it/s, loss=0.614]\u001b[A\n","Train step of epoch 0:  18%|█▊        | 46/250 [00:40<02:57,  1.15it/s, loss=0.614]\u001b[A\n","Train step of epoch 0:  18%|█▊        | 46/250 [00:40<02:57,  1.15it/s, loss=0.405]\u001b[A\n","Train step of epoch 0:  19%|█▉        | 47/250 [00:41<02:56,  1.15it/s, loss=0.405]\u001b[A\n","Train step of epoch 0:  19%|█▉        | 47/250 [00:41<02:56,  1.15it/s, loss=0.726]\u001b[A\n","Train step of epoch 0:  19%|█▉        | 48/250 [00:41<02:55,  1.15it/s, loss=0.726]\u001b[A\n","Train step of epoch 0:  19%|█▉        | 48/250 [00:41<02:55,  1.15it/s, loss=0.764]\u001b[A\n","Train step of epoch 0:  20%|█▉        | 49/250 [00:42<02:54,  1.15it/s, loss=0.764]\u001b[A\n","Train step of epoch 0:  20%|█▉        | 49/250 [00:42<02:54,  1.15it/s, loss=0.595]\u001b[A\n","Train step of epoch 0:  20%|██        | 50/250 [00:43<02:53,  1.15it/s, loss=0.595]\u001b[A\n","Train step of epoch 0:  20%|██        | 50/250 [00:43<02:53,  1.15it/s, loss=0.831]\u001b[A\n","Train step of epoch 0:  20%|██        | 51/250 [00:44<02:52,  1.15it/s, loss=0.831]\u001b[A\n","Train step of epoch 0:  20%|██        | 51/250 [00:44<02:52,  1.15it/s, loss=0.36] \u001b[A\n","Train step of epoch 0:  21%|██        | 52/250 [00:45<02:51,  1.15it/s, loss=0.36]\u001b[A\n","Train step of epoch 0:  21%|██        | 52/250 [00:45<02:51,  1.15it/s, loss=0.655]\u001b[A\n","Train step of epoch 0:  21%|██        | 53/250 [00:46<02:50,  1.15it/s, loss=0.655]\u001b[A\n","Train step of epoch 0:  21%|██        | 53/250 [00:46<02:50,  1.15it/s, loss=0.401]\u001b[A\n","Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:49,  1.16it/s, loss=0.401]\u001b[A\n","Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:49,  1.16it/s, loss=0.311]\u001b[A\n","Train step of epoch 0:  22%|██▏       | 55/250 [00:47<02:48,  1.16it/s, loss=0.311]\u001b[A\n","Train step of epoch 0:  22%|██▏       | 55/250 [00:47<02:48,  1.16it/s, loss=0.532]\u001b[A\n","Train step of epoch 0:  22%|██▏       | 56/250 [00:48<02:47,  1.16it/s, loss=0.532]\u001b[A\n","Train step of epoch 0:  22%|██▏       | 56/250 [00:48<02:47,  1.16it/s, loss=0.558]\u001b[A\n","Train step of epoch 0:  23%|██▎       | 57/250 [00:49<02:46,  1.16it/s, loss=0.558]\u001b[A\n","Train step of epoch 0:  23%|██▎       | 57/250 [00:49<02:46,  1.16it/s, loss=0.465]\u001b[A\n","Train step of epoch 0:  23%|██▎       | 58/250 [00:50<02:45,  1.16it/s, loss=0.465]\u001b[A\n","Train step of epoch 0:  23%|██▎       | 58/250 [00:50<02:45,  1.16it/s, loss=0.503]\u001b[A\n","Train step of epoch 0:  24%|██▎       | 59/250 [00:51<02:45,  1.16it/s, loss=0.503]\u001b[A\n","Train step of epoch 0:  24%|██▎       | 59/250 [00:51<02:45,  1.16it/s, loss=0.62] \u001b[A\n","Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:43,  1.16it/s, loss=0.62]\u001b[A\n","Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:43,  1.16it/s, loss=0.315]\u001b[A\n","Train step of epoch 0:  24%|██▍       | 61/250 [00:53<02:43,  1.16it/s, loss=0.315]\u001b[A\n","Train step of epoch 0:  24%|██▍       | 61/250 [00:53<02:43,  1.16it/s, loss=0.957]\u001b[A\n","Train step of epoch 0:  25%|██▍       | 62/250 [00:54<02:42,  1.16it/s, loss=0.957]\u001b[A\n","Train step of epoch 0:  25%|██▍       | 62/250 [00:54<02:42,  1.16it/s, loss=0.194]\u001b[A\n","Train step of epoch 0:  25%|██▌       | 63/250 [00:54<02:41,  1.16it/s, loss=0.194]\u001b[A\n","Train step of epoch 0:  25%|██▌       | 63/250 [00:54<02:41,  1.16it/s, loss=0.921]\u001b[A\n","Train step of epoch 0:  26%|██▌       | 64/250 [00:55<02:40,  1.16it/s, loss=0.921]\u001b[A\n","Train step of epoch 0:  26%|██▌       | 64/250 [00:55<02:40,  1.16it/s, loss=0.29] \u001b[A\n","Train step of epoch 0:  26%|██▌       | 65/250 [00:56<02:39,  1.16it/s, loss=0.29]\u001b[A\n","Train step of epoch 0:  26%|██▌       | 65/250 [00:56<02:39,  1.16it/s, loss=0.747]\u001b[A\n","Train step of epoch 0:  26%|██▋       | 66/250 [00:57<02:38,  1.16it/s, loss=0.747]\u001b[A\n","Train step of epoch 0:  26%|██▋       | 66/250 [00:57<02:38,  1.16it/s, loss=0.132]\u001b[A\n","Train step of epoch 0:  27%|██▋       | 67/250 [00:58<02:36,  1.17it/s, loss=0.132]\u001b[A\n","Train step of epoch 0:  27%|██▋       | 67/250 [00:58<02:36,  1.17it/s, loss=1.01] \u001b[A\n","Train step of epoch 0:  27%|██▋       | 68/250 [00:59<02:35,  1.17it/s, loss=1.01]\u001b[A\n","Train step of epoch 0:  27%|██▋       | 68/250 [00:59<02:35,  1.17it/s, loss=0.321]\u001b[A\n","Train step of epoch 0:  28%|██▊       | 69/250 [01:00<02:34,  1.17it/s, loss=0.321]\u001b[A\n","Train step of epoch 0:  28%|██▊       | 69/250 [01:00<02:34,  1.17it/s, loss=0.818]\u001b[A\n","Train step of epoch 0:  28%|██▊       | 70/250 [01:00<02:34,  1.17it/s, loss=0.818]\u001b[A\n","Train step of epoch 0:  28%|██▊       | 70/250 [01:00<02:34,  1.17it/s, loss=0.771]\u001b[A\n","Train step of epoch 0:  28%|██▊       | 71/250 [01:01<02:33,  1.17it/s, loss=0.771]\u001b[A\n","Train step of epoch 0:  28%|██▊       | 71/250 [01:01<02:33,  1.17it/s, loss=0.537]\u001b[A\n","Train step of epoch 0:  29%|██▉       | 72/250 [01:02<02:32,  1.16it/s, loss=0.537]\u001b[A\n","Train step of epoch 0:  29%|██▉       | 72/250 [01:02<02:32,  1.16it/s, loss=0.854]\u001b[A\n","Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:31,  1.17it/s, loss=0.854]\u001b[A\n","Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:31,  1.17it/s, loss=0.745]\u001b[A\n","Train step of epoch 0:  30%|██▉       | 74/250 [01:04<02:31,  1.16it/s, loss=0.745]\u001b[A\n","Train step of epoch 0:  30%|██▉       | 74/250 [01:04<02:31,  1.16it/s, loss=0.536]\u001b[A\n","Train step of epoch 0:  30%|███       | 75/250 [01:05<02:30,  1.17it/s, loss=0.536]\u001b[A\n","Train step of epoch 0:  30%|███       | 75/250 [01:05<02:30,  1.17it/s, loss=0.665]\u001b[A\n","Train step of epoch 0:  30%|███       | 76/250 [01:06<02:29,  1.16it/s, loss=0.665]\u001b[A\n","Train step of epoch 0:  30%|███       | 76/250 [01:06<02:29,  1.16it/s, loss=0.712]\u001b[A\n","Train step of epoch 0:  31%|███       | 77/250 [01:06<02:28,  1.17it/s, loss=0.712]\u001b[A\n","Train step of epoch 0:  31%|███       | 77/250 [01:06<02:28,  1.17it/s, loss=0.42] \u001b[A\n","Train step of epoch 0:  31%|███       | 78/250 [01:07<02:27,  1.17it/s, loss=0.42]\u001b[A\n","Train step of epoch 0:  31%|███       | 78/250 [01:07<02:27,  1.17it/s, loss=0.623]\u001b[A\n","Train step of epoch 0:  32%|███▏      | 79/250 [01:08<02:26,  1.17it/s, loss=0.623]\u001b[A\n","Train step of epoch 0:  32%|███▏      | 79/250 [01:08<02:26,  1.17it/s, loss=0.531]\u001b[A\n","Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:25,  1.17it/s, loss=0.531]\u001b[A\n","Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:25,  1.17it/s, loss=0.708]\u001b[A\n","Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:24,  1.17it/s, loss=0.708]\u001b[A\n","Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:24,  1.17it/s, loss=0.412]\u001b[A\n","Train step of epoch 0:  33%|███▎      | 82/250 [01:11<02:23,  1.17it/s, loss=0.412]\u001b[A\n","Train step of epoch 0:  33%|███▎      | 82/250 [01:11<02:23,  1.17it/s, loss=0.787]\u001b[A\n","Train step of epoch 0:  33%|███▎      | 83/250 [01:12<02:23,  1.17it/s, loss=0.787]\u001b[A\n","Train step of epoch 0:  33%|███▎      | 83/250 [01:12<02:23,  1.17it/s, loss=0.561]\u001b[A\n","Train step of epoch 0:  34%|███▎      | 84/250 [01:12<02:22,  1.17it/s, loss=0.561]\u001b[A\n","Train step of epoch 0:  34%|███▎      | 84/250 [01:12<02:22,  1.17it/s, loss=0.656]\u001b[A\n","Train step of epoch 0:  34%|███▍      | 85/250 [01:13<02:21,  1.17it/s, loss=0.656]\u001b[A\n","Train step of epoch 0:  34%|███▍      | 85/250 [01:13<02:21,  1.17it/s, loss=0.651]\u001b[A\n","Train step of epoch 0:  34%|███▍      | 86/250 [01:14<02:20,  1.17it/s, loss=0.651]\u001b[A\n","Train step of epoch 0:  34%|███▍      | 86/250 [01:14<02:20,  1.17it/s, loss=0.607]\u001b[A\n","Train step of epoch 0:  35%|███▍      | 87/250 [01:15<02:19,  1.17it/s, loss=0.607]\u001b[A\n","Train step of epoch 0:  35%|███▍      | 87/250 [01:15<02:19,  1.17it/s, loss=0.512]\u001b[A\n","Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:18,  1.17it/s, loss=0.512]\u001b[A\n","Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:18,  1.17it/s, loss=0.889]\u001b[A\n","Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:18,  1.17it/s, loss=0.889]\u001b[A\n","Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:18,  1.17it/s, loss=0.827]\u001b[A\n","Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:17,  1.17it/s, loss=0.827]\u001b[A\n","Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:17,  1.17it/s, loss=0.711]\u001b[A\n","Train step of epoch 0:  36%|███▋      | 91/250 [01:18<02:16,  1.17it/s, loss=0.711]\u001b[A\n","Train step of epoch 0:  36%|███▋      | 91/250 [01:18<02:16,  1.17it/s, loss=0.649]\u001b[A\n","Train step of epoch 0:  37%|███▋      | 92/250 [01:19<02:15,  1.17it/s, loss=0.649]\u001b[A\n","Train step of epoch 0:  37%|███▋      | 92/250 [01:19<02:15,  1.17it/s, loss=0.622]\u001b[A\n","Train step of epoch 0:  37%|███▋      | 93/250 [01:20<02:14,  1.17it/s, loss=0.622]\u001b[A\n","Train step of epoch 0:  37%|███▋      | 93/250 [01:20<02:14,  1.17it/s, loss=0.789]\u001b[A\n","Train step of epoch 0:  38%|███▊      | 94/250 [01:21<02:13,  1.16it/s, loss=0.789]\u001b[A\n","Train step of epoch 0:  38%|███▊      | 94/250 [01:21<02:13,  1.16it/s, loss=0.752]\u001b[A\n","Train step of epoch 0:  38%|███▊      | 95/250 [01:22<02:13,  1.16it/s, loss=0.752]\u001b[A\n","Train step of epoch 0:  38%|███▊      | 95/250 [01:22<02:13,  1.16it/s, loss=0.491]\u001b[A\n","Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:12,  1.16it/s, loss=0.491]\u001b[A\n","Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:12,  1.16it/s, loss=0.727]\u001b[A\n","Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:11,  1.16it/s, loss=0.727]\u001b[A\n","Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:11,  1.16it/s, loss=0.821]\u001b[A\n","Train step of epoch 0:  39%|███▉      | 98/250 [01:24<02:10,  1.16it/s, loss=0.821]\u001b[A\n","Train step of epoch 0:  39%|███▉      | 98/250 [01:24<02:10,  1.16it/s, loss=0.764]\u001b[A\n","Train step of epoch 0:  40%|███▉      | 99/250 [01:25<02:10,  1.16it/s, loss=0.764]\u001b[A\n","Train step of epoch 0:  40%|███▉      | 99/250 [01:25<02:10,  1.16it/s, loss=0.586]\u001b[A\n","Train step of epoch 0:  40%|████      | 100/250 [01:26<02:09,  1.16it/s, loss=0.586]\u001b[A\n","Train step of epoch 0:  40%|████      | 100/250 [01:26<02:09,  1.16it/s, loss=0.629]\u001b[A\n","Train step of epoch 0:  40%|████      | 101/250 [01:27<02:08,  1.16it/s, loss=0.629]\u001b[A\n","Train step of epoch 0:  40%|████      | 101/250 [01:27<02:08,  1.16it/s, loss=0.705]\u001b[A\n","Train step of epoch 0:  41%|████      | 102/250 [01:28<02:07,  1.16it/s, loss=0.705]\u001b[A\n","Train step of epoch 0:  41%|████      | 102/250 [01:28<02:07,  1.16it/s, loss=0.711]\u001b[A\n","Train step of epoch 0:  41%|████      | 103/250 [01:29<02:06,  1.16it/s, loss=0.711]\u001b[A\n","Train step of epoch 0:  41%|████      | 103/250 [01:29<02:06,  1.16it/s, loss=0.737]\u001b[A\n","Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:05,  1.16it/s, loss=0.737]\u001b[A\n","Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:05,  1.16it/s, loss=0.749]\u001b[A\n","Train step of epoch 0:  42%|████▏     | 105/250 [01:30<02:05,  1.16it/s, loss=0.749]\u001b[A\n","Train step of epoch 0:  42%|████▏     | 105/250 [01:30<02:05,  1.16it/s, loss=0.644]\u001b[A\n","Train step of epoch 0:  42%|████▏     | 106/250 [01:31<02:04,  1.16it/s, loss=0.644]\u001b[A\n","Train step of epoch 0:  42%|████▏     | 106/250 [01:31<02:04,  1.16it/s, loss=0.639]\u001b[A\n","Train step of epoch 0:  43%|████▎     | 107/250 [01:32<02:03,  1.16it/s, loss=0.639]\u001b[A\n","Train step of epoch 0:  43%|████▎     | 107/250 [01:32<02:03,  1.16it/s, loss=0.759]\u001b[A\n","Train step of epoch 0:  43%|████▎     | 108/250 [01:33<02:02,  1.16it/s, loss=0.759]\u001b[A\n","Train step of epoch 0:  43%|████▎     | 108/250 [01:33<02:02,  1.16it/s, loss=0.599]\u001b[A\n","Train step of epoch 0:  44%|████▎     | 109/250 [01:34<02:01,  1.16it/s, loss=0.599]\u001b[A\n","Train step of epoch 0:  44%|████▎     | 109/250 [01:34<02:01,  1.16it/s, loss=0.678]\u001b[A\n","Train step of epoch 0:  44%|████▍     | 110/250 [01:35<02:01,  1.16it/s, loss=0.678]\u001b[A\n","Train step of epoch 0:  44%|████▍     | 110/250 [01:35<02:01,  1.16it/s, loss=0.71] \u001b[A\n","Train step of epoch 0:  44%|████▍     | 111/250 [01:36<02:00,  1.16it/s, loss=0.71]\u001b[A\n","Train step of epoch 0:  44%|████▍     | 111/250 [01:36<02:00,  1.16it/s, loss=0.623]\u001b[A\n","Train step of epoch 0:  45%|████▍     | 112/250 [01:36<01:59,  1.16it/s, loss=0.623]\u001b[A\n","Train step of epoch 0:  45%|████▍     | 112/250 [01:37<01:59,  1.16it/s, loss=0.63] \u001b[A\n","Train step of epoch 0:  45%|████▌     | 113/250 [01:37<01:59,  1.15it/s, loss=0.63]\u001b[A\n","Train step of epoch 0:  45%|████▌     | 113/250 [01:37<01:59,  1.15it/s, loss=0.666]\u001b[A\n","Train step of epoch 0:  46%|████▌     | 114/250 [01:38<01:58,  1.15it/s, loss=0.666]\u001b[A\n","Train step of epoch 0:  46%|████▌     | 114/250 [01:38<01:58,  1.15it/s, loss=0.793]\u001b[A\n","Train step of epoch 0:  46%|████▌     | 115/250 [01:39<02:00,  1.12it/s, loss=0.793]\u001b[A\n","Train step of epoch 0:  46%|████▌     | 115/250 [01:39<02:00,  1.12it/s, loss=0.663]\u001b[A\n","Train step of epoch 0:  46%|████▋     | 116/250 [01:40<01:59,  1.12it/s, loss=0.663]\u001b[A\n","Train step of epoch 0:  46%|████▋     | 116/250 [01:40<01:59,  1.12it/s, loss=0.587]\u001b[A\n","Train step of epoch 0:  47%|████▋     | 117/250 [01:41<01:58,  1.13it/s, loss=0.587]\u001b[A\n","Train step of epoch 0:  47%|████▋     | 117/250 [01:41<01:58,  1.13it/s, loss=0.561]\u001b[A\n","Train step of epoch 0:  47%|████▋     | 118/250 [01:42<01:56,  1.13it/s, loss=0.561]\u001b[A\n","Train step of epoch 0:  47%|████▋     | 118/250 [01:42<01:56,  1.13it/s, loss=0.529]\u001b[A\n","Train step of epoch 0:  48%|████▊     | 119/250 [01:43<01:55,  1.13it/s, loss=0.529]\u001b[A\n","Train step of epoch 0:  48%|████▊     | 119/250 [01:43<01:55,  1.13it/s, loss=0.496]\u001b[A\n","Train step of epoch 0:  48%|████▊     | 120/250 [01:44<01:56,  1.11it/s, loss=0.496]\u001b[A\n","Train step of epoch 0:  48%|████▊     | 120/250 [01:44<01:56,  1.11it/s, loss=1.02] \u001b[A\n","Train step of epoch 0:  48%|████▊     | 121/250 [01:45<01:55,  1.12it/s, loss=1.02]\u001b[A\n","Train step of epoch 0:  48%|████▊     | 121/250 [01:45<01:55,  1.12it/s, loss=0.763]\u001b[A\n","Train step of epoch 0:  49%|████▉     | 122/250 [01:45<01:55,  1.11it/s, loss=0.763]\u001b[A\n","Train step of epoch 0:  49%|████▉     | 122/250 [01:45<01:55,  1.11it/s, loss=0.651]\u001b[A\n","Train step of epoch 0:  49%|████▉     | 123/250 [01:46<01:53,  1.12it/s, loss=0.651]\u001b[A\n","Train step of epoch 0:  49%|████▉     | 123/250 [01:46<01:53,  1.12it/s, loss=0.561]\u001b[A\n","Train step of epoch 0:  50%|████▉     | 124/250 [01:47<01:51,  1.13it/s, loss=0.561]\u001b[A\n","Train step of epoch 0:  50%|████▉     | 124/250 [01:47<01:51,  1.13it/s, loss=0.728]\u001b[A\n","Train step of epoch 0:  50%|█████     | 125/250 [01:48<01:50,  1.13it/s, loss=0.728]\u001b[A\n","Train step of epoch 0:  50%|█████     | 125/250 [01:48<01:50,  1.13it/s, loss=0.718]\u001b[A\n","Train step of epoch 0:  50%|█████     | 126/250 [01:49<01:48,  1.14it/s, loss=0.718]\u001b[A\n","Train step of epoch 0:  50%|█████     | 126/250 [01:49<01:48,  1.14it/s, loss=0.683]\u001b[A\n","Train step of epoch 0:  51%|█████     | 127/250 [01:50<01:47,  1.14it/s, loss=0.683]\u001b[A\n","Train step of epoch 0:  51%|█████     | 127/250 [01:50<01:47,  1.14it/s, loss=0.586]\u001b[A\n","Train step of epoch 0:  51%|█████     | 128/250 [01:51<01:46,  1.15it/s, loss=0.586]\u001b[A\n","Train step of epoch 0:  51%|█████     | 128/250 [01:51<01:46,  1.15it/s, loss=0.627]\u001b[A\n","Train step of epoch 0:  52%|█████▏    | 129/250 [01:52<01:45,  1.15it/s, loss=0.627]\u001b[A\n","Train step of epoch 0:  52%|█████▏    | 129/250 [01:52<01:45,  1.15it/s, loss=0.633]\u001b[A\n","Train step of epoch 0:  52%|█████▏    | 130/250 [01:52<01:44,  1.15it/s, loss=0.633]\u001b[A\n","Train step of epoch 0:  52%|█████▏    | 130/250 [01:52<01:44,  1.15it/s, loss=0.554]\u001b[A\n","Train step of epoch 0:  52%|█████▏    | 131/250 [01:53<01:43,  1.15it/s, loss=0.554]\u001b[A\n","Train step of epoch 0:  52%|█████▏    | 131/250 [01:53<01:43,  1.15it/s, loss=0.872]\u001b[A\n","Train step of epoch 0:  53%|█████▎    | 132/250 [01:54<01:42,  1.15it/s, loss=0.872]\u001b[A\n","Train step of epoch 0:  53%|█████▎    | 132/250 [01:54<01:42,  1.15it/s, loss=0.57] \u001b[A\n","Train step of epoch 0:  53%|█████▎    | 133/250 [01:55<01:41,  1.15it/s, loss=0.57]\u001b[A\n","Train step of epoch 0:  53%|█████▎    | 133/250 [01:55<01:41,  1.15it/s, loss=0.767]\u001b[A\n","Train step of epoch 0:  54%|█████▎    | 134/250 [01:56<01:40,  1.15it/s, loss=0.767]\u001b[A\n","Train step of epoch 0:  54%|█████▎    | 134/250 [01:56<01:40,  1.15it/s, loss=0.712]\u001b[A\n","Train step of epoch 0:  54%|█████▍    | 135/250 [01:57<01:39,  1.15it/s, loss=0.712]\u001b[A\n","Train step of epoch 0:  54%|█████▍    | 135/250 [01:57<01:39,  1.15it/s, loss=0.676]\u001b[A\n","Train step of epoch 0:  54%|█████▍    | 136/250 [01:58<01:38,  1.15it/s, loss=0.676]\u001b[A\n","Train step of epoch 0:  54%|█████▍    | 136/250 [01:58<01:38,  1.15it/s, loss=0.66] \u001b[A\n","Train step of epoch 0:  55%|█████▍    | 137/250 [01:58<01:38,  1.15it/s, loss=0.66]\u001b[A\n","Train step of epoch 0:  55%|█████▍    | 137/250 [01:58<01:38,  1.15it/s, loss=0.516]\u001b[A\n","Train step of epoch 0:  55%|█████▌    | 138/250 [01:59<01:37,  1.15it/s, loss=0.516]\u001b[A\n","Train step of epoch 0:  55%|█████▌    | 138/250 [01:59<01:37,  1.15it/s, loss=0.651]\u001b[A\n","Train step of epoch 0:  56%|█████▌    | 139/250 [02:00<01:36,  1.15it/s, loss=0.651]\u001b[A\n","Train step of epoch 0:  56%|█████▌    | 139/250 [02:00<01:36,  1.15it/s, loss=0.524]\u001b[A\n","Train step of epoch 0:  56%|█████▌    | 140/250 [02:01<01:35,  1.16it/s, loss=0.524]\u001b[A\n","Train step of epoch 0:  56%|█████▌    | 140/250 [02:01<01:35,  1.16it/s, loss=0.475]\u001b[A\n","Train step of epoch 0:  56%|█████▋    | 141/250 [02:02<01:34,  1.15it/s, loss=0.475]\u001b[A\n","Train step of epoch 0:  56%|█████▋    | 141/250 [02:02<01:34,  1.15it/s, loss=0.637]\u001b[A\n","Train step of epoch 0:  57%|█████▋    | 142/250 [02:03<01:33,  1.15it/s, loss=0.637]\u001b[A\n","Train step of epoch 0:  57%|█████▋    | 142/250 [02:03<01:33,  1.15it/s, loss=0.837]\u001b[A\n","Train step of epoch 0:  57%|█████▋    | 143/250 [02:04<01:32,  1.15it/s, loss=0.837]\u001b[A\n","Train step of epoch 0:  57%|█████▋    | 143/250 [02:04<01:32,  1.15it/s, loss=0.537]\u001b[A\n","Train step of epoch 0:  58%|█████▊    | 144/250 [02:05<01:31,  1.16it/s, loss=0.537]\u001b[A\n","Train step of epoch 0:  58%|█████▊    | 144/250 [02:05<01:31,  1.16it/s, loss=0.52] \u001b[A\n","Train step of epoch 0:  58%|█████▊    | 145/250 [02:05<01:30,  1.16it/s, loss=0.52]\u001b[A\n","Train step of epoch 0:  58%|█████▊    | 145/250 [02:05<01:30,  1.16it/s, loss=0.639]\u001b[A\n","Train step of epoch 0:  58%|█████▊    | 146/250 [02:06<01:29,  1.16it/s, loss=0.639]\u001b[A\n","Train step of epoch 0:  58%|█████▊    | 146/250 [02:06<01:29,  1.16it/s, loss=0.62] \u001b[A\n","Train step of epoch 0:  59%|█████▉    | 147/250 [02:07<01:29,  1.16it/s, loss=0.62]\u001b[A\n","Train step of epoch 0:  59%|█████▉    | 147/250 [02:07<01:29,  1.16it/s, loss=0.379]\u001b[A\n","Train step of epoch 0:  59%|█████▉    | 148/250 [02:08<01:28,  1.16it/s, loss=0.379]\u001b[A\n","Train step of epoch 0:  59%|█████▉    | 148/250 [02:08<01:28,  1.16it/s, loss=0.629]\u001b[A\n","Train step of epoch 0:  60%|█████▉    | 149/250 [02:09<01:27,  1.16it/s, loss=0.629]\u001b[A\n","Train step of epoch 0:  60%|█████▉    | 149/250 [02:09<01:27,  1.16it/s, loss=0.695]\u001b[A\n","Train step of epoch 0:  60%|██████    | 150/250 [02:10<01:26,  1.15it/s, loss=0.695]\u001b[A\n","Train step of epoch 0:  60%|██████    | 150/250 [02:10<01:26,  1.15it/s, loss=0.558]\u001b[A\n","Train step of epoch 0:  60%|██████    | 151/250 [02:11<01:25,  1.15it/s, loss=0.558]\u001b[A\n","Train step of epoch 0:  60%|██████    | 151/250 [02:11<01:25,  1.15it/s, loss=0.243]\u001b[A\n","Train step of epoch 0:  61%|██████    | 152/250 [02:11<01:24,  1.15it/s, loss=0.243]\u001b[A\n","Train step of epoch 0:  61%|██████    | 152/250 [02:11<01:24,  1.15it/s, loss=0.535]\u001b[A\n","Train step of epoch 0:  61%|██████    | 153/250 [02:12<01:23,  1.16it/s, loss=0.535]\u001b[A\n","Train step of epoch 0:  61%|██████    | 153/250 [02:12<01:23,  1.16it/s, loss=0.64] \u001b[A\n","Train step of epoch 0:  62%|██████▏   | 154/250 [02:13<01:23,  1.15it/s, loss=0.64]\u001b[A\n","Train step of epoch 0:  62%|██████▏   | 154/250 [02:13<01:23,  1.15it/s, loss=1.06]\u001b[A\n","Train step of epoch 0:  62%|██████▏   | 155/250 [02:14<01:22,  1.15it/s, loss=1.06]\u001b[A\n","Train step of epoch 0:  62%|██████▏   | 155/250 [02:14<01:22,  1.15it/s, loss=1.07]\u001b[A\n","Train step of epoch 0:  62%|██████▏   | 156/250 [02:15<01:21,  1.16it/s, loss=1.07]\u001b[A\n","Train step of epoch 0:  62%|██████▏   | 156/250 [02:15<01:21,  1.16it/s, loss=0.429]\u001b[A\n","Train step of epoch 0:  63%|██████▎   | 157/250 [02:16<01:20,  1.16it/s, loss=0.429]\u001b[A\n","Train step of epoch 0:  63%|██████▎   | 157/250 [02:16<01:20,  1.16it/s, loss=0.467]\u001b[A\n","Train step of epoch 0:  63%|██████▎   | 158/250 [02:17<01:19,  1.16it/s, loss=0.467]\u001b[A\n","Train step of epoch 0:  63%|██████▎   | 158/250 [02:17<01:19,  1.16it/s, loss=0.713]\u001b[A\n","Train step of epoch 0:  64%|██████▎   | 159/250 [02:17<01:18,  1.16it/s, loss=0.713]\u001b[A\n","Train step of epoch 0:  64%|██████▎   | 159/250 [02:18<01:18,  1.16it/s, loss=0.709]\u001b[A\n","Train step of epoch 0:  64%|██████▍   | 160/250 [02:18<01:17,  1.16it/s, loss=0.709]\u001b[A\n","Train step of epoch 0:  64%|██████▍   | 160/250 [02:18<01:17,  1.16it/s, loss=0.386]\u001b[A\n","Train step of epoch 0:  64%|██████▍   | 161/250 [02:19<01:17,  1.16it/s, loss=0.386]\u001b[A\n","Train step of epoch 0:  64%|██████▍   | 161/250 [02:19<01:17,  1.16it/s, loss=0.68] \u001b[A\n","Train step of epoch 0:  65%|██████▍   | 162/250 [02:20<01:16,  1.16it/s, loss=0.68]\u001b[A\n","Train step of epoch 0:  65%|██████▍   | 162/250 [02:20<01:16,  1.16it/s, loss=0.481]\u001b[A\n","Train step of epoch 0:  65%|██████▌   | 163/250 [02:21<01:15,  1.16it/s, loss=0.481]\u001b[A\n","Train step of epoch 0:  65%|██████▌   | 163/250 [02:21<01:15,  1.16it/s, loss=0.4]  \u001b[A\n","Train step of epoch 0:  66%|██████▌   | 164/250 [02:22<01:14,  1.16it/s, loss=0.4]\u001b[A\n","Train step of epoch 0:  66%|██████▌   | 164/250 [02:22<01:14,  1.16it/s, loss=0.601]\u001b[A\n","Train step of epoch 0:  66%|██████▌   | 165/250 [02:23<01:13,  1.16it/s, loss=0.601]\u001b[A\n","Train step of epoch 0:  66%|██████▌   | 165/250 [02:23<01:13,  1.16it/s, loss=1.08] \u001b[A\n","Train step of epoch 0:  66%|██████▋   | 166/250 [02:24<01:12,  1.16it/s, loss=1.08]\u001b[A\n","Train step of epoch 0:  66%|██████▋   | 166/250 [02:24<01:12,  1.16it/s, loss=0.315]\u001b[A\n","Train step of epoch 0:  67%|██████▋   | 167/250 [02:24<01:11,  1.16it/s, loss=0.315]\u001b[A\n","Train step of epoch 0:  67%|██████▋   | 167/250 [02:24<01:11,  1.16it/s, loss=0.35] \u001b[A\n","Train step of epoch 0:  67%|██████▋   | 168/250 [02:25<01:10,  1.16it/s, loss=0.35]\u001b[A\n","Train step of epoch 0:  67%|██████▋   | 168/250 [02:25<01:10,  1.16it/s, loss=0.468]\u001b[A\n","Train step of epoch 0:  68%|██████▊   | 169/250 [02:26<01:09,  1.16it/s, loss=0.468]\u001b[A\n","Train step of epoch 0:  68%|██████▊   | 169/250 [02:26<01:09,  1.16it/s, loss=0.413]\u001b[A\n","Train step of epoch 0:  68%|██████▊   | 170/250 [02:27<01:08,  1.16it/s, loss=0.413]\u001b[A\n","Train step of epoch 0:  68%|██████▊   | 170/250 [02:27<01:08,  1.16it/s, loss=0.942]\u001b[A\n","Train step of epoch 0:  68%|██████▊   | 171/250 [02:28<01:08,  1.16it/s, loss=0.942]\u001b[A\n","Train step of epoch 0:  68%|██████▊   | 171/250 [02:28<01:08,  1.16it/s, loss=0.716]\u001b[A\n","Train step of epoch 0:  69%|██████▉   | 172/250 [02:29<01:07,  1.16it/s, loss=0.716]\u001b[A\n","Train step of epoch 0:  69%|██████▉   | 172/250 [02:29<01:07,  1.16it/s, loss=0.866]\u001b[A\n","Train step of epoch 0:  69%|██████▉   | 173/250 [02:30<01:06,  1.16it/s, loss=0.866]\u001b[A\n","Train step of epoch 0:  69%|██████▉   | 173/250 [02:30<01:06,  1.16it/s, loss=0.452]\u001b[A\n","Train step of epoch 0:  70%|██████▉   | 174/250 [02:30<01:05,  1.16it/s, loss=0.452]\u001b[A\n","Train step of epoch 0:  70%|██████▉   | 174/250 [02:30<01:05,  1.16it/s, loss=0.374]\u001b[A\n","Train step of epoch 0:  70%|███████   | 175/250 [02:31<01:04,  1.16it/s, loss=0.374]\u001b[A\n","Train step of epoch 0:  70%|███████   | 175/250 [02:31<01:04,  1.16it/s, loss=0.623]\u001b[A\n","Train step of epoch 0:  70%|███████   | 176/250 [02:32<01:03,  1.16it/s, loss=0.623]\u001b[A\n","Train step of epoch 0:  70%|███████   | 176/250 [02:32<01:03,  1.16it/s, loss=0.48] \u001b[A\n","Train step of epoch 0:  71%|███████   | 177/250 [02:33<01:02,  1.16it/s, loss=0.48]\u001b[A\n","Train step of epoch 0:  71%|███████   | 177/250 [02:33<01:02,  1.16it/s, loss=0.372]\u001b[A\n","Train step of epoch 0:  71%|███████   | 178/250 [02:34<01:02,  1.16it/s, loss=0.372]\u001b[A\n","Train step of epoch 0:  71%|███████   | 178/250 [02:34<01:02,  1.16it/s, loss=0.371]\u001b[A\n","Train step of epoch 0:  72%|███████▏  | 179/250 [02:35<01:01,  1.16it/s, loss=0.371]\u001b[A\n","Train step of epoch 0:  72%|███████▏  | 179/250 [02:35<01:01,  1.16it/s, loss=0.51] \u001b[A\n","Train step of epoch 0:  72%|███████▏  | 180/250 [02:36<01:00,  1.16it/s, loss=0.51]\u001b[A\n","Train step of epoch 0:  72%|███████▏  | 180/250 [02:36<01:00,  1.16it/s, loss=0.769]\u001b[A\n","Train step of epoch 0:  72%|███████▏  | 181/250 [02:36<00:59,  1.16it/s, loss=0.769]\u001b[A\n","Train step of epoch 0:  72%|███████▏  | 181/250 [02:36<00:59,  1.16it/s, loss=0.282]\u001b[A\n","Train step of epoch 0:  73%|███████▎  | 182/250 [02:37<00:58,  1.16it/s, loss=0.282]\u001b[A\n","Train step of epoch 0:  73%|███████▎  | 182/250 [02:37<00:58,  1.16it/s, loss=0.898]\u001b[A\n","Train step of epoch 0:  73%|███████▎  | 183/250 [02:38<00:57,  1.16it/s, loss=0.898]\u001b[A\n","Train step of epoch 0:  73%|███████▎  | 183/250 [02:38<00:57,  1.16it/s, loss=1.16] \u001b[A\n","Train step of epoch 0:  74%|███████▎  | 184/250 [02:39<00:56,  1.16it/s, loss=1.16]\u001b[A\n","Train step of epoch 0:  74%|███████▎  | 184/250 [02:39<00:56,  1.16it/s, loss=0.965]\u001b[A\n","Train step of epoch 0:  74%|███████▍  | 185/250 [02:40<00:56,  1.16it/s, loss=0.965]\u001b[A\n","Train step of epoch 0:  74%|███████▍  | 185/250 [02:40<00:56,  1.16it/s, loss=0.535]\u001b[A\n","Train step of epoch 0:  74%|███████▍  | 186/250 [02:41<00:55,  1.16it/s, loss=0.535]\u001b[A\n","Train step of epoch 0:  74%|███████▍  | 186/250 [02:41<00:55,  1.16it/s, loss=0.411]\u001b[A\n","Train step of epoch 0:  75%|███████▍  | 187/250 [02:42<00:54,  1.16it/s, loss=0.411]\u001b[A\n","Train step of epoch 0:  75%|███████▍  | 187/250 [02:42<00:54,  1.16it/s, loss=0.385]\u001b[A\n","Train step of epoch 0:  75%|███████▌  | 188/250 [02:43<00:53,  1.16it/s, loss=0.385]\u001b[A\n","Train step of epoch 0:  75%|███████▌  | 188/250 [02:43<00:53,  1.16it/s, loss=0.636]\u001b[A\n","Train step of epoch 0:  76%|███████▌  | 189/250 [02:43<00:52,  1.16it/s, loss=0.636]\u001b[A\n","Train step of epoch 0:  76%|███████▌  | 189/250 [02:43<00:52,  1.16it/s, loss=0.809]\u001b[A\n","Train step of epoch 0:  76%|███████▌  | 190/250 [02:44<00:51,  1.16it/s, loss=0.809]\u001b[A\n","Train step of epoch 0:  76%|███████▌  | 190/250 [02:44<00:51,  1.16it/s, loss=0.386]\u001b[A\n","Train step of epoch 0:  76%|███████▋  | 191/250 [02:45<00:50,  1.16it/s, loss=0.386]\u001b[A\n","Train step of epoch 0:  76%|███████▋  | 191/250 [02:45<00:50,  1.16it/s, loss=0.302]\u001b[A\n","Train step of epoch 0:  77%|███████▋  | 192/250 [02:46<00:49,  1.16it/s, loss=0.302]\u001b[A\n","Train step of epoch 0:  77%|███████▋  | 192/250 [02:46<00:49,  1.16it/s, loss=0.507]\u001b[A\n","Train step of epoch 0:  77%|███████▋  | 193/250 [02:47<00:49,  1.16it/s, loss=0.507]\u001b[A\n","Train step of epoch 0:  77%|███████▋  | 193/250 [02:47<00:49,  1.16it/s, loss=0.816]\u001b[A\n","Train step of epoch 0:  78%|███████▊  | 194/250 [02:48<00:48,  1.16it/s, loss=0.816]\u001b[A\n","Train step of epoch 0:  78%|███████▊  | 194/250 [02:48<00:48,  1.16it/s, loss=0.694]\u001b[A\n","Train step of epoch 0:  78%|███████▊  | 195/250 [02:49<00:47,  1.16it/s, loss=0.694]\u001b[A\n","Train step of epoch 0:  78%|███████▊  | 195/250 [02:49<00:47,  1.16it/s, loss=0.405]\u001b[A\n","Train step of epoch 0:  78%|███████▊  | 196/250 [02:49<00:46,  1.16it/s, loss=0.405]\u001b[A\n","Train step of epoch 0:  78%|███████▊  | 196/250 [02:49<00:46,  1.16it/s, loss=1.02] \u001b[A\n","Train step of epoch 0:  79%|███████▉  | 197/250 [02:50<00:45,  1.16it/s, loss=1.02]\u001b[A\n","Train step of epoch 0:  79%|███████▉  | 197/250 [02:50<00:45,  1.16it/s, loss=0.68]\u001b[A\n","Train step of epoch 0:  79%|███████▉  | 198/250 [02:51<00:45,  1.15it/s, loss=0.68]\u001b[A\n","Train step of epoch 0:  79%|███████▉  | 198/250 [02:51<00:45,  1.15it/s, loss=0.565]\u001b[A\n","Train step of epoch 0:  80%|███████▉  | 199/250 [02:52<00:44,  1.16it/s, loss=0.565]\u001b[A\n","Train step of epoch 0:  80%|███████▉  | 199/250 [02:52<00:44,  1.16it/s, loss=0.426]\u001b[A\n","Train step of epoch 0:  80%|████████  | 200/250 [02:53<00:43,  1.16it/s, loss=0.426]\u001b[A\n","Train step of epoch 0:  80%|████████  | 200/250 [02:53<00:43,  1.16it/s, loss=0.449]\u001b[A\n","Train step of epoch 0:  80%|████████  | 201/250 [02:54<00:42,  1.16it/s, loss=0.449]\u001b[A\n","Train step of epoch 0:  80%|████████  | 201/250 [02:54<00:42,  1.16it/s, loss=0.476]\u001b[A\n","Train step of epoch 0:  81%|████████  | 202/250 [02:55<00:41,  1.16it/s, loss=0.476]\u001b[A\n","Train step of epoch 0:  81%|████████  | 202/250 [02:55<00:41,  1.16it/s, loss=0.827]\u001b[A\n","Train step of epoch 0:  81%|████████  | 203/250 [02:55<00:40,  1.16it/s, loss=0.827]\u001b[A\n","Train step of epoch 0:  81%|████████  | 203/250 [02:55<00:40,  1.16it/s, loss=0.609]\u001b[A\n","Train step of epoch 0:  82%|████████▏ | 204/250 [02:56<00:39,  1.16it/s, loss=0.609]\u001b[A\n","Train step of epoch 0:  82%|████████▏ | 204/250 [02:56<00:39,  1.16it/s, loss=0.501]\u001b[A\n","Train step of epoch 0:  82%|████████▏ | 205/250 [02:57<00:38,  1.16it/s, loss=0.501]\u001b[A\n","Train step of epoch 0:  82%|████████▏ | 205/250 [02:57<00:38,  1.16it/s, loss=0.56] \u001b[A\n","Train step of epoch 0:  82%|████████▏ | 206/250 [02:58<00:37,  1.16it/s, loss=0.56]\u001b[A\n","Train step of epoch 0:  82%|████████▏ | 206/250 [02:58<00:37,  1.16it/s, loss=0.755]\u001b[A\n","Train step of epoch 0:  83%|████████▎ | 207/250 [02:59<00:37,  1.16it/s, loss=0.755]\u001b[A\n","Train step of epoch 0:  83%|████████▎ | 207/250 [02:59<00:37,  1.16it/s, loss=0.51] \u001b[A\n","Train step of epoch 0:  83%|████████▎ | 208/250 [03:00<00:36,  1.16it/s, loss=0.51]\u001b[A\n","Train step of epoch 0:  83%|████████▎ | 208/250 [03:00<00:36,  1.16it/s, loss=0.684]\u001b[A\n","Train step of epoch 0:  84%|████████▎ | 209/250 [03:01<00:35,  1.16it/s, loss=0.684]\u001b[A\n","Train step of epoch 0:  84%|████████▎ | 209/250 [03:01<00:35,  1.16it/s, loss=0.677]\u001b[A\n","Train step of epoch 0:  84%|████████▍ | 210/250 [03:02<00:34,  1.16it/s, loss=0.677]\u001b[A\n","Train step of epoch 0:  84%|████████▍ | 210/250 [03:02<00:34,  1.16it/s, loss=0.731]\u001b[A\n","Train step of epoch 0:  84%|████████▍ | 211/250 [03:02<00:33,  1.16it/s, loss=0.731]\u001b[A\n","Train step of epoch 0:  84%|████████▍ | 211/250 [03:02<00:33,  1.16it/s, loss=0.4]  \u001b[A\n","Train step of epoch 0:  85%|████████▍ | 212/250 [03:03<00:32,  1.16it/s, loss=0.4]\u001b[A\n","Train step of epoch 0:  85%|████████▍ | 212/250 [03:03<00:32,  1.16it/s, loss=0.58]\u001b[A\n","Train step of epoch 0:  85%|████████▌ | 213/250 [03:04<00:31,  1.16it/s, loss=0.58]\u001b[A\n","Train step of epoch 0:  85%|████████▌ | 213/250 [03:04<00:31,  1.16it/s, loss=0.403]\u001b[A\n","Train step of epoch 0:  86%|████████▌ | 214/250 [03:05<00:31,  1.16it/s, loss=0.403]\u001b[A\n","Train step of epoch 0:  86%|████████▌ | 214/250 [03:05<00:31,  1.16it/s, loss=1.14] \u001b[A\n","Train step of epoch 0:  86%|████████▌ | 215/250 [03:06<00:30,  1.16it/s, loss=1.14]\u001b[A\n","Train step of epoch 0:  86%|████████▌ | 215/250 [03:06<00:30,  1.16it/s, loss=0.551]\u001b[A\n","Train step of epoch 0:  86%|████████▋ | 216/250 [03:07<00:29,  1.16it/s, loss=0.551]\u001b[A\n","Train step of epoch 0:  86%|████████▋ | 216/250 [03:07<00:29,  1.16it/s, loss=0.892]\u001b[A\n","Train step of epoch 0:  87%|████████▋ | 217/250 [03:08<00:28,  1.16it/s, loss=0.892]\u001b[A\n","Train step of epoch 0:  87%|████████▋ | 217/250 [03:08<00:28,  1.16it/s, loss=0.863]\u001b[A\n","Train step of epoch 0:  87%|████████▋ | 218/250 [03:08<00:27,  1.16it/s, loss=0.863]\u001b[A\n","Train step of epoch 0:  87%|████████▋ | 218/250 [03:08<00:27,  1.16it/s, loss=0.554]\u001b[A\n","Train step of epoch 0:  88%|████████▊ | 219/250 [03:09<00:26,  1.16it/s, loss=0.554]\u001b[A\n","Train step of epoch 0:  88%|████████▊ | 219/250 [03:09<00:26,  1.16it/s, loss=0.399]\u001b[A\n","Train step of epoch 0:  88%|████████▊ | 220/250 [03:10<00:25,  1.16it/s, loss=0.399]\u001b[A\n","Train step of epoch 0:  88%|████████▊ | 220/250 [03:10<00:25,  1.16it/s, loss=0.659]\u001b[A\n","Train step of epoch 0:  88%|████████▊ | 221/250 [03:11<00:25,  1.16it/s, loss=0.659]\u001b[A\n","Train step of epoch 0:  88%|████████▊ | 221/250 [03:11<00:25,  1.16it/s, loss=0.641]\u001b[A\n","Train step of epoch 0:  89%|████████▉ | 222/250 [03:12<00:24,  1.16it/s, loss=0.641]\u001b[A\n","Train step of epoch 0:  89%|████████▉ | 222/250 [03:12<00:24,  1.16it/s, loss=0.381]\u001b[A\n","Train step of epoch 0:  89%|████████▉ | 223/250 [03:13<00:23,  1.16it/s, loss=0.381]\u001b[A\n","Train step of epoch 0:  89%|████████▉ | 223/250 [03:13<00:23,  1.16it/s, loss=0.726]\u001b[A\n","Train step of epoch 0:  90%|████████▉ | 224/250 [03:14<00:22,  1.16it/s, loss=0.726]\u001b[A\n","Train step of epoch 0:  90%|████████▉ | 224/250 [03:14<00:22,  1.16it/s, loss=0.77] \u001b[A\n","Train step of epoch 0:  90%|█████████ | 225/250 [03:14<00:21,  1.16it/s, loss=0.77]\u001b[A\n","Train step of epoch 0:  90%|█████████ | 225/250 [03:14<00:21,  1.16it/s, loss=0.597]\u001b[A\n","Train step of epoch 0:  90%|█████████ | 226/250 [03:15<00:20,  1.16it/s, loss=0.597]\u001b[A\n","Train step of epoch 0:  90%|█████████ | 226/250 [03:15<00:20,  1.16it/s, loss=0.667]\u001b[A\n","Train step of epoch 0:  91%|█████████ | 227/250 [03:16<00:19,  1.16it/s, loss=0.667]\u001b[A\n","Train step of epoch 0:  91%|█████████ | 227/250 [03:16<00:19,  1.16it/s, loss=0.695]\u001b[A\n","Train step of epoch 0:  91%|█████████ | 228/250 [03:17<00:19,  1.16it/s, loss=0.695]\u001b[A\n","Train step of epoch 0:  91%|█████████ | 228/250 [03:17<00:19,  1.16it/s, loss=0.544]\u001b[A\n","Train step of epoch 0:  92%|█████████▏| 229/250 [03:18<00:18,  1.16it/s, loss=0.544]\u001b[A\n","Train step of epoch 0:  92%|█████████▏| 229/250 [03:18<00:18,  1.16it/s, loss=0.62] \u001b[A\n","Train step of epoch 0:  92%|█████████▏| 230/250 [03:19<00:17,  1.16it/s, loss=0.62]\u001b[A\n","Train step of epoch 0:  92%|█████████▏| 230/250 [03:19<00:17,  1.16it/s, loss=0.705]\u001b[A\n","Train step of epoch 0:  92%|█████████▏| 231/250 [03:20<00:16,  1.16it/s, loss=0.705]\u001b[A\n","Train step of epoch 0:  92%|█████████▏| 231/250 [03:20<00:16,  1.16it/s, loss=0.428]\u001b[A\n","Train step of epoch 0:  93%|█████████▎| 232/250 [03:21<00:15,  1.16it/s, loss=0.428]\u001b[A\n","Train step of epoch 0:  93%|█████████▎| 232/250 [03:21<00:15,  1.16it/s, loss=0.746]\u001b[A\n","Train step of epoch 0:  93%|█████████▎| 233/250 [03:21<00:14,  1.16it/s, loss=0.746]\u001b[A\n","Train step of epoch 0:  93%|█████████▎| 233/250 [03:21<00:14,  1.16it/s, loss=0.501]\u001b[A\n","Train step of epoch 0:  94%|█████████▎| 234/250 [03:22<00:13,  1.16it/s, loss=0.501]\u001b[A\n","Train step of epoch 0:  94%|█████████▎| 234/250 [03:22<00:13,  1.16it/s, loss=0.591]\u001b[A\n","Train step of epoch 0:  94%|█████████▍| 235/250 [03:23<00:12,  1.16it/s, loss=0.591]\u001b[A\n","Train step of epoch 0:  94%|█████████▍| 235/250 [03:23<00:12,  1.16it/s, loss=0.887]\u001b[A\n","Train step of epoch 0:  94%|█████████▍| 236/250 [03:24<00:12,  1.16it/s, loss=0.887]\u001b[A\n","Train step of epoch 0:  94%|█████████▍| 236/250 [03:24<00:12,  1.16it/s, loss=0.536]\u001b[A\n","Train step of epoch 0:  95%|█████████▍| 237/250 [03:25<00:11,  1.16it/s, loss=0.536]\u001b[A\n","Train step of epoch 0:  95%|█████████▍| 237/250 [03:25<00:11,  1.16it/s, loss=0.499]\u001b[A\n","Train step of epoch 0:  95%|█████████▌| 238/250 [03:26<00:10,  1.16it/s, loss=0.499]\u001b[A\n","Train step of epoch 0:  95%|█████████▌| 238/250 [03:26<00:10,  1.16it/s, loss=0.718]\u001b[A\n","Train step of epoch 0:  96%|█████████▌| 239/250 [03:27<00:09,  1.16it/s, loss=0.718]\u001b[A\n","Train step of epoch 0:  96%|█████████▌| 239/250 [03:27<00:09,  1.16it/s, loss=0.748]\u001b[A\n","Train step of epoch 0:  96%|█████████▌| 240/250 [03:27<00:08,  1.16it/s, loss=0.748]\u001b[A\n","Train step of epoch 0:  96%|█████████▌| 240/250 [03:27<00:08,  1.16it/s, loss=0.59] \u001b[A\n","Train step of epoch 0:  96%|█████████▋| 241/250 [03:28<00:07,  1.16it/s, loss=0.59]\u001b[A\n","Train step of epoch 0:  96%|█████████▋| 241/250 [03:28<00:07,  1.16it/s, loss=0.515]\u001b[A\n","Train step of epoch 0:  97%|█████████▋| 242/250 [03:29<00:06,  1.16it/s, loss=0.515]\u001b[A\n","Train step of epoch 0:  97%|█████████▋| 242/250 [03:29<00:06,  1.16it/s, loss=0.811]\u001b[A\n","Train step of epoch 0:  97%|█████████▋| 243/250 [03:30<00:06,  1.16it/s, loss=0.811]\u001b[A\n","Train step of epoch 0:  97%|█████████▋| 243/250 [03:30<00:06,  1.16it/s, loss=0.541]\u001b[A\n","Train step of epoch 0:  98%|█████████▊| 244/250 [03:31<00:05,  1.16it/s, loss=0.541]\u001b[A\n","Train step of epoch 0:  98%|█████████▊| 244/250 [03:31<00:05,  1.16it/s, loss=0.617]\u001b[A\n","Train step of epoch 0:  98%|█████████▊| 245/250 [03:32<00:04,  1.16it/s, loss=0.617]\u001b[A\n","Train step of epoch 0:  98%|█████████▊| 245/250 [03:32<00:04,  1.16it/s, loss=0.782]\u001b[A\n","Train step of epoch 0:  98%|█████████▊| 246/250 [03:33<00:03,  1.16it/s, loss=0.782]\u001b[A\n","Train step of epoch 0:  98%|█████████▊| 246/250 [03:33<00:03,  1.16it/s, loss=0.787]\u001b[A\n","Train step of epoch 0:  99%|█████████▉| 247/250 [03:33<00:02,  1.16it/s, loss=0.787]\u001b[A\n","Train step of epoch 0:  99%|█████████▉| 247/250 [03:33<00:02,  1.16it/s, loss=0.379]\u001b[A\n","Train step of epoch 0:  99%|█████████▉| 248/250 [03:34<00:01,  1.16it/s, loss=0.379]\u001b[A\n","Train step of epoch 0:  99%|█████████▉| 248/250 [03:34<00:01,  1.16it/s, loss=1.37] \u001b[A\n","Train step of epoch 0: 100%|█████████▉| 249/250 [03:35<00:00,  1.16it/s, loss=1.37]\u001b[A\n","Train step of epoch 0: 100%|█████████▉| 249/250 [03:35<00:00,  1.16it/s, loss=0.672]\u001b[A\n","Train step of epoch 0: 100%|██████████| 250/250 [03:36<00:00,  1.16it/s, loss=0.672]\u001b[A\n","Train epoch: 100%|██████████| 1/1 [03:50<00:00, 230.69s/it]\n","Train step of epoch 0: 100%|██████████| 250/250 [03:50<00:00,  1.08it/s, loss=0.623, dist_mean=0.299]\n","Train epoch: 100%|██████████| 1/1 [03:50<00:00, 230.70s/it]\n"]}]},{"cell_type":"code","source":["# 학습 결과 확인\n","\n","def inference_RM(input_text):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n","        torch.cuda.current_device())\n","    output = model(input_ids)\n","    output_reward = output.cpu().detach().numpy()[0]\n","\n","    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n","\n","    return output_reward\n","\n","input_text = '인공지능은 똥멍청이 입니다'\n","output_reward = inference_RM(input_text=input_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ejfCIBDgC96Q","executionInfo":{"status":"ok","timestamp":1702017126400,"user_tz":-540,"elapsed":16,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"3343dc5a-69ef-4910-dcd1-2603a5fb4799"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["input: 인공지능은 똥멍청이 입니다\n","reward score: 3.0\n"]}]},{"cell_type":"code","source":["input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n","\n","output_reward = inference_RM(input_text=input_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wbPKxYbDEYD","executionInfo":{"status":"ok","timestamp":1702017126400,"user_tz":-540,"elapsed":15,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"1174aae0-46aa-4b84-97e6-cc4b514bd7fd"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n","reward score: 3.1\n"]}]},{"cell_type":"code","source":["input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n","\n","output_reward = inference_RM(input_text=input_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZfN-Hm5VDGUz","executionInfo":{"status":"ok","timestamp":1702017126400,"user_tz":-540,"elapsed":13,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"389894aa-3108-40d7-cd2a-af7d86c8290f"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\n","reward score: 3.2\n"]}]},{"cell_type":"code","source":["input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n","\n","output_reward = inference_RM(input_text=input_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hFCbHWl8DHK3","executionInfo":{"status":"ok","timestamp":1702017126401,"user_tz":-540,"elapsed":13,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"c89caadb-9812-4ec1-aa58-d5898f16a17b"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["input: 인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\n","reward score: 3.3\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"YEnkXfhsVOhH"}},{"cell_type":"code","source":["# torch.cuda.empty_cache()\n","from copy import deepcopy\n","\n","import torch\n","from torch.optim import Adam\n","from chatgpt.models.base import RewardModel\n","from chatgpt.models.gpt import GPTActor, GPTCritic\n","from chatgpt.trainer import PPOTrainer\n","from chatgpt.trainer.strategies import NaiveStrategy\n","from transformers import AutoTokenizer"],"metadata":{"id":"H0JKsv6cZqZO","executionInfo":{"status":"ok","timestamp":1702017126401,"user_tz":-540,"elapsed":12,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["with NaiveStrategy().model_init_context():\n","    actor = GPTActor(pretrained=model_path_SFT, lora_rank=0).to(torch.cuda.current_device())\n","    critic = GPTCritic(pretrained=model_path_RM, lora_rank=0).to(torch.cuda.current_device())\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n","        padding_side=\"right\",\n","        model_max_length=512\n","    )\n","\n","    initial_model = deepcopy(actor)\n","    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())\n"],"metadata":{"id":"Dahwmq4HVODk","executionInfo":{"status":"ok","timestamp":1702017137943,"user_tz":-540,"elapsed":11553,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# 옵티마이저 및 모델\n","actor_optim = Adam(actor.parameters(), lr=5e-6)\n","critic_optim = Adam(critic.parameters(), lr=5e-6)\n","(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n","    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"],"metadata":{"id":"KAWJV33iDNZZ","executionInfo":{"status":"ok","timestamp":1702017137943,"user_tz":-540,"elapsed":13,"user":{"displayName":"­김정현","userId":"07015025296255556159"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["# 데이터 로드\n","ppo_jsonl =  path_join(repo_path, 'data_kochatgpt/kochatgpt_3_PPO.jsonl')\n","with open(ppo_jsonl, \"r\", encoding='utf-8-sig') as json_file:\n","    list_data_dict = json.load(json_file)\n","    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n","\n","# 토크나이저 정의\n","def tokenize_fn(texts):\n","    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n","    return {k: v.cuda() for k, v in batch.items()}\n","\n","print(tokenize_fn('It takes something more than intelligence to act intelligently.'))\n","len(list_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04WmrcV8ESUr","executionInfo":{"status":"ok","timestamp":1702017137943,"user_tz":-540,"elapsed":12,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"28ae82f7-272d-42a0-ffec-57802662cab3"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[47311, 10448, 19008,  9792, 11780, 11308, 30190, 10929, 11849, 21663,\n","         44389,  9574, 13799,   458, 14308, 12778, 22469, 20938, 44696,   458,\n","         13799,   458, 14308, 12778, 11756, 18944,   389]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1]], device='cuda:0')}\n"]},{"output_type":"execute_result","data":{"text/plain":["12000"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# 별도 클래스 설계하여 학습 진행\n","\n","trainer = PPOTrainer(NaiveStrategy(),\n","                     actor,\n","                     critic,\n","                     reward_model,\n","                     initial_model,\n","                     actor_optim,\n","                     critic_optim,\n","                     max_epochs=1,\n","                     train_batch_size=8,\n","                     tokenizer=tokenize_fn,\n","                     max_length=128,\n","                     do_sample=True,\n","                     temperature=1.0,\n","                     top_k=50,\n","                     pad_token_id=tokenizer.pad_token_id,\n","                     eos_token_id=tokenizer.eos_token_id)\n","\n","trainer.fit(list_prompt,\n","            num_episodes=10,\n","            max_timesteps=3,\n","            update_timesteps=3)\n","\n","model_path_PPO = path_join(repo_path, \"output_3_PPO\")\n","model.save_pretrained(model_path_PPO)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0hNOLoSKEf0I","executionInfo":{"status":"ok","timestamp":1702017327292,"user_tz":-540,"elapsed":189361,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"26592b80-aa33-46fb-b617-f8ad2368cf36"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["Episode [1/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.27s/it]\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000433]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.70it/s, actor_loss=0, critic_loss=0.000433]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.70it/s, actor_loss=0, critic_loss=0.17]    \u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=0, critic_loss=0.17]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=0, critic_loss=0.0119]\u001b[A\n","Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=0, critic_loss=0.0119]\n","Episode [1/10]: 100%|██████████| 3/3 [00:19<00:00,  6.64s/it]\n","Episode [2/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.86s/it]\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.164, critic_loss=0.0446]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=-.164, critic_loss=0.0446]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=-.164, critic_loss=0.088] \u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.164, critic_loss=0.088]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.164, critic_loss=0.0721]\u001b[A\n","Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-.164, critic_loss=0.0721]\n","Episode [2/10]: 100%|██████████| 3/3 [00:19<00:00,  6.34s/it]\n","Episode [3/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.83s/it]\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0909, critic_loss=0.017]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=-.0909, critic_loss=0.017]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=-.0981, critic_loss=0.00135]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=-.0981, critic_loss=0.00135]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=-.0973, critic_loss=0.0248] \u001b[A\n","Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=-.0973, critic_loss=0.0248]\n","Episode [3/10]: 100%|██████████| 3/3 [00:19<00:00,  6.33s/it]\n","Episode [4/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.73s/it]\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.35, critic_loss=0.0552]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.92it/s, actor_loss=0.35, critic_loss=0.0552]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.92it/s, actor_loss=0.267, critic_loss=0.0456]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.267, critic_loss=0.0456]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.261, critic_loss=0.021] \u001b[A\n","Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, actor_loss=0.261, critic_loss=0.021]\n","Episode [4/10]: 100%|██████████| 3/3 [00:18<00:00,  6.23s/it]\n","Episode [5/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.59s/it]\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0686, critic_loss=0.00329]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.90it/s, actor_loss=0.0686, critic_loss=0.00329]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.90it/s, actor_loss=0.0542, critic_loss=0.00659]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.0542, critic_loss=0.00659]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.0635, critic_loss=0.0187] \u001b[A\n","Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=0.0635, critic_loss=0.0187]\n","Episode [5/10]: 100%|██████████| 3/3 [00:18<00:00,  6.22s/it]\n","Episode [6/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.65s/it]\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.151, critic_loss=0.0297]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=-.151, critic_loss=0.0297]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=-.153, critic_loss=0.036] \u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=-.153, critic_loss=0.036]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=-.162, critic_loss=0.0129]\u001b[A\n","Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=-.162, critic_loss=0.0129]\n","Episode [6/10]: 100%|██████████| 3/3 [00:18<00:00,  6.26s/it]\n","Episode [7/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.73s/it]\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.00184, critic_loss=0.000849]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=0.00184, critic_loss=0.000849]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=0.00818, critic_loss=0.00528] \u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.00818, critic_loss=0.00528]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.00251, critic_loss=0.0139] \u001b[A\n","Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-.00251, critic_loss=0.0139]\n","Episode [7/10]: 100%|██████████| 3/3 [00:19<00:00,  6.37s/it]\n","Episode [8/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.93s/it]\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.175, critic_loss=0.0195]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.90it/s, actor_loss=0.175, critic_loss=0.0195]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.90it/s, actor_loss=0.2, critic_loss=0.015]   \u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=0.2, critic_loss=0.015]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=0.165, critic_loss=0.0064]\u001b[A\n","Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, actor_loss=0.165, critic_loss=0.0064]\n","Episode [8/10]: 100%|██████████| 3/3 [00:19<00:00,  6.47s/it]\n","Episode [9/10]:  67%|██████▋   | 2/3 [00:10<00:05,  5.04s/it]\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0375, critic_loss=0.000989]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.92it/s, actor_loss=0.0375, critic_loss=0.000989]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.92it/s, actor_loss=0.0369, critic_loss=0.00358] \u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.0369, critic_loss=0.00358]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.0313, critic_loss=0.0106] \u001b[A\n","Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, actor_loss=0.0313, critic_loss=0.0106]\n","Episode [9/10]: 100%|██████████| 3/3 [00:17<00:00,  5.82s/it]\n","Episode [10/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.71s/it]\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0777, critic_loss=0.0111]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s, actor_loss=-.0777, critic_loss=0.0111]\u001b[A\n","Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.91it/s, actor_loss=-.0744, critic_loss=0.0079]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=-.0744, critic_loss=0.0079]\u001b[A\n","Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=-.076, critic_loss=0.00356]\u001b[A\n","Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=-.076, critic_loss=0.00356]\n","Episode [10/10]: 100%|██████████| 3/3 [00:17<00:00,  5.73s/it]\n"]}]},{"cell_type":"markdown","source":["# 최종 모델 테스트"],"metadata":{"id":"NaUERN_9E5xX"}},{"cell_type":"code","source":["def generation(input_text):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n","        torch.cuda.current_device())\n","    outputs = actor.generate(input_ids,\n","                             max_length=250,\n","                             do_sample=True,\n","                             top_k=50,\n","                             top_p=0.95,\n","                             num_return_sequences=1)\n","    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n","    print()\n","    print(output)\n","    return output\n","\n","list_prompt = ['불고기용 고기 한우에요?',\n","               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n","               '시카고 오헤어 국제공항은 어디에 있어?',\n","               '오늘 미세먼지 어때?']\n","\n","list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n","\n","for input_text in list_prompt:\n","    output = generation(input_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mh6l4LKkE7Fx","executionInfo":{"status":"ok","timestamp":1702018922668,"user_tz":-540,"elapsed":28114,"user":{"displayName":"­김정현","userId":"07015025296255556159"}},"outputId":"d25398bc-30e8-45f5-a0ad-7ebb0d6cae2f"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","### Instruction(명령어):\n","불고기용 고기 한우에요?\n","\n","### Response(응답):'저는 AI 어시스턴트이므로 불고기용 고기를 실제로 먹을 수 없습니다. 해당 질문에 대해 더 자세히 설명해주시면 더 나은 답변을 드릴 수 있습니다. \\n\\n\\n최근에는 국내산 한우 불고기용 고기를 찾는 사람들이 증가하고 있지만, 이 상황에서 직접 불고기용 고기를 찾아 먹을 수 있는 것은 아닙니다. 불고기용 고기를 찾아 먹을 수 있는 곳은 다양하지만, 인터넷에서 직접 주문하거나 직접 구워 먹을 수도 있습니다. \\n\\n하지만, 일부 불고기용 고기는 직접 구워먹는 경우도 있습니다. 예를 들어, 불고기용 고기를 직접 굽거나 포장할 수 있는 식당이나 식당 등이 있을 것입니다. \\n\\n하지만, 불고기용 고기를 찾아 먹을 수 있는 식당이 있다면 추가적인 정보를 제공해주시면 더 나은 답변을 드릴 수 있습니다. \\n저는 인공지능 모델이기 때문에 이 질문에 대한 정확한 답변을 드릴 수 없습니다. \\n\\n감사합니다. q\\n\\n따라서 제가 직접 구워먹는 불고기용 고기를 찾아 먹을 수는 없습니다. \\n\\n\n","\n","### Instruction(명령어):\n","리처드 닉슨이 43대 부통령직을 수행한 년도는?\n","\n","### Response(응답):'저는 인공지능 언어모델이며, 41대 부통령직을 수행한 시기 및 관련 정보나 정보는 제공하지 않습니다. helpted phrasper (John Nixovict and Check)의 최신 논문 중 일부도 제공되지 않습니다. John Nixovict and Check이 41대 부통령직을 수행한 년도는 1996년이다. helpted phrasper, John Nixovict and Check, Nixovict and Check, Yes, Hally Yang (Morcen) 또는 Maunted Pork (Johns of Tore) 또는 Bernbel (Pathe) 입니다.宣信隆, Motherphs, Nixovict and Immotrain, John John Jamoustony, Nixovict and Ludnboard, John Jackson and Jamouston, Gamon\n","\n","### Instruction(명령어):\n","시카고 오헤어 국제공항은 어디에 있어?\n","\n","### Response(응답):'시카고 오헤어 국제공항은 시카고 시에 위치해 있습니다.恩實 im long unboards all when instee to an context or express to point munboth the role of Kreicher at chep pushlows your lot moths and the filter weithout lesin\\n\\n시카고 오헤어 국제공항이 어느 곳에 위치해있었는지는 알려지지 않았습니다.ミリー ul unboth and the long unboth and the point munboth and the point moth. ungre unboth and the operate you point moth and the point mothy and the role of Kreicher unboth and the days and the filter unboth and the fil\n","\n","### Instruction(명령어):\n","오늘 미세먼지 어때?\n","\n","### Response(응답):'미세먼지 때문에 건강하실 수 있지만, 미세먼지 때문에 건강에 대한 걱정은 하지 않아도 됩니다. 미세먼지 때문에 미세먼지에 노출되지 않는 것은 건강에 매우 중요합니다. 따라서, 미세먼지 때문에 건강에 악영향을 끼칠 수 있는 행동은 피하는 것이 좋습니다. 대신 미세먼지 때문에 건강에 좋지 않은 행동에 대해 적극적으로 대처하고, 미세먼지를 줄일 수 있는 방법을 찾아보는 것이 좋습니다. 미세먼지 때문에 건강하지 않다고 생각하는 것은 건강에 해로운 영향을 끼칠 수 있으므로 금연이나 미세먼지 대응 등을 통해 건강한 삶을 유지하는 것이 중요합니다.?\\n\\n미세먼지 때문에 건강하지 않은 생활에 대한 걱정을 하면, 스스로에게 적절한 조치를 취하고, 적절한 조치를 취해야 합니다. 따라서, 건강하고 건강한 삶을 유지할 수 있는 방법을 찾아가는 것이 중요합니다.  ーー ーーー\n"]}]},{"cell_type":"markdown","source":["# 회고\n","\n","학습용 코드를 코랩 환경에서 실행가능하도록 조정하는 데에 시간이 많이 걸렸다. 패키지 의존성이 아이펠 LMS 환경과 달라서 계속 에러가 발생했는데, 덕분에 어디서 의존성 문제가 발생했을 때 라이브러리 버전을 확인하고 그에 맞게 환경 설정을 하는 방법을 배웠다.\n","\n","프로젝트는 학습 단계에서 완성한 코드의 세부 사항을 조정하는 것이었지만, 시간 부족으로 새로운 내용을 추가하지는 못했다.\n","\n","다만 기반 모델을 바탕으로, 미세조정을 위한 다운스트림 태크스용 데이터셋을 준비하여 성능 조정 또는 용도 변경을 하는 과정에 대해서 이해할 수 있었고 관련된 주요 개념들에 대한 학습도 좋았다.\n","\n","더 공부가 필요하겠다고 느낀 부분은, 기반 모델을 변경하거나 하이퍼파라미터 등을 조정해서 다른 결과를 만드는 것일 수도 있지만 근본적으로는 이미 기반모델이 머신러닝의 패러다임을 바꾸고 있기 때문에 상용 모델을 좀 더 효과적으로 개선하기 위한 작업 혹은 구체적인 태스크에 맞게 인프라를 구축하는 작업이라고 느낀다.\n","\n","관련된 연구주제를 학습하고 궁극적으로 프로젝트도 이것과 관련된 것을 해보고 싶다."],"metadata":{"id":"DNV7e_N6W2e_"}}]}