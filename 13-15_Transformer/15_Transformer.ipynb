{"cells":[{"cell_type":"markdown","metadata":{"id":"tknSEXfMeLHB"},"source":["# Seq2seq 프로젝트: 한영 번역기 만들기"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12122,"status":"ok","timestamp":1700135921567,"user":{"displayName":"­김정현","userId":"07015025296255556159"},"user_tz":-540},"id":"KDJ1SY3lPlWe","outputId":"2b7df3ce-0c32-4d54-b5ac-64259b9bf220"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  fonts-nanum\n","0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n","Need to get 10.3 MB of archives.\n","After this operation, 34.1 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n","Fetched 10.3 MB in 2s (5,578 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package fonts-nanum.\n","(Reading database ... 120882 files and directories currently installed.)\n","Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n","Unpacking fonts-nanum (20200506-1) ...\n","Setting up fonts-nanum (20200506-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n","/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n","/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n","/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n","/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n","/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n","/root/.local/share/fonts: skipping, no such directory\n","/root/.fonts: skipping, no such directory\n","/usr/share/fonts/truetype: skipping, looped directory detected\n","/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n","/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n","/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n","/var/cache/fontconfig: cleaning cache directory\n","/root/.cache/fontconfig: not cleaning non-existent cache directory\n","/root/.fontconfig: not cleaning non-existent cache directory\n","fc-cache: succeeded\n"]}],"source":["# 한글폰트 설치: 실행후 런타임 재시작 필요\n","!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1627,"status":"ok","timestamp":1700135923193,"user":{"displayName":"­김정현","userId":"07015025296255556159"},"user_tz":-540},"id":"MglIYUTrPpAD","outputId":"b03a590b-2818-45a6-9062-6153b1f34a21"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47800 (\\N{HANGUL SYLLABLE MOM}) missing from current font.\n","  fig.canvas.print_figure(bytes_io, **kw)\n","/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47924 (\\N{HANGUL SYLLABLE MU}) missing from current font.\n","  fig.canvas.print_figure(bytes_io, **kw)\n","/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44172 (\\N{HANGUL SYLLABLE GE}) missing from current font.\n","  fig.canvas.print_figure(bytes_io, **kw)\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 53412 (\\N{HANGUL SYLLABLE KI}) missing from current font.\n","  fig.canvas.print_figure(bytes_io, **kw)\n","/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50752 (\\N{HANGUL SYLLABLE WA}) missing from current font.\n","  fig.canvas.print_figure(bytes_io, **kw)\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n","WARNING:matplotlib.font_manager:findfont: Font family 'NanumBarunGothic' not found.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# 한글 표시 여부 확인\n","import matplotlib.pyplot as plt\n","plt.rcParams['font.family'] = 'NanumBarunGothic' # 나눔바른고딕 적용하기\n","\n","# 예제 데이터\n","heights = [150, 160, 170, 180, 190]  # 키 (cm)\n","weights = [50, 60, 70, 80, 90]       # 몸무게 (kg)\n","\n","# 산점도 그리기\n","plt.scatter(heights, weights)\n","\n","# 제목 및 레이블 추가\n","plt.title(\"키와 몸무게\")\n","plt.xlabel(\"키 (cm)\")\n","plt.ylabel(\"몸무게 (kg)\")\n","\n","# 그래프 표시\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"F7v_DaPKfCoL"},"source":["## 데이터 정제"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3439,"status":"ok","timestamp":1700135926626,"user":{"displayName":"­김정현","userId":"07015025296255556159"},"user_tz":-540},"id":"SyGlHAI0T4gL","outputId":"8db5bebd-b706-46c8-f8fe-4b7722e6ae94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'korean-parallel-corpora'...\n","remote: Enumerating objects: 173, done.\u001b[K\n","remote: Counting objects: 100% (42/42), done.\u001b[K\n","remote: Compressing objects: 100% (38/38), done.\u001b[K\n","remote: Total 173 (delta 18), reused 0 (delta 0), pack-reused 131\u001b[K\n","Receiving objects: 100% (173/173), 20.48 MiB | 11.78 MiB/s, done.\n","Resolving deltas: 100% (61/61), done.\n"]}],"source":["# 학습 데이터 다운로드\n","!git clone https://github.com/jungyeul/korean-parallel-corpora.git\n","\n","# 한-영 뉴스 코퍼스 압축해제\n","!mkdir kor-eng-train\n","!tar -xzf ./korean-parallel-corpora/korean-english-news-v1/korean-english-park.train.tar.gz -C kor-eng-train/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":491,"status":"ok","timestamp":1700135927113,"user":{"displayName":"­김정현","userId":"07015025296255556159"},"user_tz":-540},"id":"bpxdJEsAfv9B","outputId":"8e0bc10d-02e0-487b-eb6c-12869847c762"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data Size in ./kor-eng-train/korean-english-park.train.en: 94123\n","Examples:\n",">> Much of personal computing is about \"can you top this?\"\n",">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n","Data Size in ./kor-eng-train/korean-english-park.train.ko: 94123\n","Examples:\n",">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"]}],"source":["# 데이터 확인\n","def read_and_print_examples(file_path, num_examples=100, step=20):\n","    with open(file_path, \"r\", encoding='utf-8') as f:\n","        raw = f.read().splitlines()\n","\n","    print(f\"Data Size in {file_path}: {len(raw)}\")\n","    print(\"Examples:\")\n","\n","    for sen in raw[:num_examples:step]:\n","        print(\">>\", sen)\n","\n","# 영어 파일 읽기 및 출력\n","read_and_print_examples(\"./kor-eng-train/korean-english-park.train.en\")\n","\n","# 한국어 파일 읽기 및 출력\n","read_and_print_examples(\"./kor-eng-train/korean-english-park.train.ko\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hm0UE7zahO3G","outputId":"75df9374-b3de-4252-da2f-02cb0af6828f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Mecab-ko-for-Google-Colab'...\n","remote: Enumerating objects: 138, done.\u001b[K\n","remote: Counting objects: 100% (47/47), done.\u001b[K\n","remote: Compressing objects: 100% (38/38), done.\u001b[K\n","remote: Total 138 (delta 26), reused 22 (delta 8), pack-reused 91\u001b[K\n","Receiving objects: 100% (138/138), 1.72 MiB | 10.91 MiB/s, done.\n","Resolving deltas: 100% (65/65), done.\n","Installing konlpy.....\n","Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n","Done\n","Installing mecab-0.996-ko-0.9.2.tar.gz.....\n","Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n","from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n","--2023-11-16 11:59:04--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n","Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db\n","Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNORGQOGWJ&Signature=BH0suF0E%2BUvWUhKN8jrtyMFbvas%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEEQaCXVzLWVhc3QtMSJGMEQCIFrABjlHo1cwK5p%2FgCx9rU2NlA74QKd9Xs2oPwW0SUvXAiB3U%2FzdmQ8QcEgoObsbcWcEKkpVQm3G5nIAVbriCbnd8SqwAgiN%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDk4NDUyNTEwMTE0NiIM7gkJhVkgyNPMxTaWKoQCLBYe2iKCNAWUpa%2FRqUhBl3gYq3DrJ5H4X0mE9zMyjoMUyqOZJuvbLUXmdpkrsChLe0wgywbgwrB%2FSe3hMB09d4l445sit1vbSBAvuXr6WeRUL4MlMTsEC8gSM1Zkh4p61z5IBj25Ug4AJdkTT%2Bgb54ZHza6%2BikjklicCcvzsKXGhJmM8x7YKNHHEvG0xUq0bpi36S1yWy%2FWpSxiJxeTaCooyO1RDTjjkdRFXVWmrqLWztq9xuxaOjjipDXnBg96lf0%2FlyEjC3CYLzgQFrfIhK5E4%2FnBmhfHCzMgD6swBEltrM4V%2FnxkxQ2NKgXzfbYpqFl8T2HGj73Hs%2Bi98JZQXr4Yxk3Qwp4fYqgY6ngFmGQU5PBNGzdfcxg0cOcLS0UvbXuFlwVMbzfQZYV1CSdQoOxhPuxHycnp4oEL2UKsnMTnf6pi06IabjwRNx%2FWsVnFSu6VMB%2FgQd00qk1UMc%2Bhm4vW5I4G47gKvwYEMSrSxah4Pc99UBAFJSDK8sMHEu1h1SSoVeLYlqJ7tf7VjhGxW%2BM4aOPXS6CfKUWe8NLBPpve0XNop62eUinHSiA%3D%3D&Expires=1700137647 [following]\n","--2023-11-16 11:59:05--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNORGQOGWJ&Signature=BH0suF0E%2BUvWUhKN8jrtyMFbvas%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEEQaCXVzLWVhc3QtMSJGMEQCIFrABjlHo1cwK5p%2FgCx9rU2NlA74QKd9Xs2oPwW0SUvXAiB3U%2FzdmQ8QcEgoObsbcWcEKkpVQm3G5nIAVbriCbnd8SqwAgiN%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDk4NDUyNTEwMTE0NiIM7gkJhVkgyNPMxTaWKoQCLBYe2iKCNAWUpa%2FRqUhBl3gYq3DrJ5H4X0mE9zMyjoMUyqOZJuvbLUXmdpkrsChLe0wgywbgwrB%2FSe3hMB09d4l445sit1vbSBAvuXr6WeRUL4MlMTsEC8gSM1Zkh4p61z5IBj25Ug4AJdkTT%2Bgb54ZHza6%2BikjklicCcvzsKXGhJmM8x7YKNHHEvG0xUq0bpi36S1yWy%2FWpSxiJxeTaCooyO1RDTjjkdRFXVWmrqLWztq9xuxaOjjipDXnBg96lf0%2FlyEjC3CYLzgQFrfIhK5E4%2FnBmhfHCzMgD6swBEltrM4V%2FnxkxQ2NKgXzfbYpqFl8T2HGj73Hs%2Bi98JZQXr4Yxk3Qwp4fYqgY6ngFmGQU5PBNGzdfcxg0cOcLS0UvbXuFlwVMbzfQZYV1CSdQoOxhPuxHycnp4oEL2UKsnMTnf6pi06IabjwRNx%2FWsVnFSu6VMB%2FgQd00qk1UMc%2Bhm4vW5I4G47gKvwYEMSrSxah4Pc99UBAFJSDK8sMHEu1h1SSoVeLYlqJ7tf7VjhGxW%2BM4aOPXS6CfKUWe8NLBPpve0XNop62eUinHSiA%3D%3D&Expires=1700137647\n","Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.171.19, 3.5.25.245, 16.182.103.81, ...\n","Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.171.19|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1414979 (1.3M) [application/x-tar]\n","Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n","\n","mecab-0.996-ko-0.9. 100%[===================>]   1.35M  3.62MB/s    in 0.4s    \n","\n","2023-11-16 11:59:06 (3.62 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n","\n","Done\n","Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n","Done\n","Change Directory to mecab-0.996-ko-0.9.2.......\n","installing mecab-0.996-ko-0.9.2.tar.gz........\n","configure\n","make\n"]}],"source":["# 설치 후 런타임 다시 시작 필요\n","!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n","!bash ./Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab_light_220429.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpiAQiA_gXjp"},"outputs":[],"source":["import re\n","\n","# 공통 전처리 함수\n","\n","kor_path = \"./kor-eng-train/korean-english-park.train.en\"\n","eng_path = \"./kor-eng-train/korean-english-park.train.ko\"\n","\n","# 데이터 정제 및 토큰화\n","def clean_corpus(kor_path, eng_path):\n","    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n","    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n","    assert len(kor) == len(eng)\n","\n","    cleaned_corpus = []\n","    seen_pairs = set()\n","\n","    for k, e in zip(kor, eng):\n","        if (k, e) not in seen_pairs:\n","            pair = f\"{k}\\t{e}\"  # 한국어 문장과 영어 문장을 탭으로 구분하여 저장\n","            cleaned_corpus.append(pair)\n","            seen_pairs.add((k, e))\n","\n","    return cleaned_corpus\n","\n","cleaned_corpus = clean_corpus(kor_path, eng_path)\n","\n","def preprocess_sentence(sentence):\n","    # 모든 입력을 소문자로 변환합니다.\n","    sentence = sentence.lower()\n","\n","    # 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n","    sentence = re.sub(r\"[^a-zA-Z가-힣?.!,]+\", \" \", sentence)\n","\n","    # 문장부호 양옆에 공백을 추가합니다.\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","\n","    # 문장 앞뒤의 불필요한 공백을 제거합니다.\n","    sentence = sentence.strip()\n","\n","    return sentence"]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"id":"sRU4x5GGFSCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W43JRf5QpH7L"},"outputs":[],"source":["import sentencepiece as spm\n","import os\n","\n","# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n","def generate_tokenizer(corpus, vocab_size, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n","    model_name = f\"{lang}_spm\"\n","\n","    # 텍스트 파일로 코퍼스 저장\n","    temp_file = f\"{model_name}.temp\"\n","    with open(temp_file, 'w', encoding='utf-8') as f:\n","        for line in corpus:\n","            f.write(f'{line}\\n')\n","\n","    # SentencePiece 모델 학습\n","    spm.SentencePieceTrainer.Train(\n","        f'--input={temp_file} --model_prefix={model_name} '\n","        f'--vocab_size={vocab_size} --character_coverage=1.0 '\n","        f'--pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} --unk_id={unk_id}'\n","    )\n","\n","    # 생성된 모델 로드\n","    tokenizer = spm.SentencePieceProcessor()\n","    tokenizer.Load(f\"{model_name}.model\")\n","\n","    # 임시 파일 삭제\n","    os.remove(temp_file)\n","\n","    return tokenizer\n","\n","\n","SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n","\n","eng_corpus = []\n","kor_corpus = []\n","\n","for pair in cleaned_corpus:\n","    k, e = pair.split(\"\\t\")\n","\n","    kor_corpus.append(preprocess_sentence(k))\n","    eng_corpus.append(preprocess_sentence(e))\n","\n","ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n","en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n","en_tokenizer.set_encode_extra_options(\"bos:eos\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAV8XpihpH7L"},"outputs":[],"source":["import tensorflow as tf\n","from tqdm.notebook import tqdm    # 진행 과정을 보기 위한 도구\n","\n","src_corpus = []\n","tgt_corpus = []\n","\n","assert len(kor_corpus) == len(eng_corpus)\n","\n","# 토큰의 길이가 50 이하인 문장만 남깁니다.\n","for idx in tqdm(range(len(kor_corpus))):\n","    src_tok = ko_tokenizer.EncodeAsIds(kor_corpus[idx])  # 한국어 문장 토큰화\n","    tgt_tok = en_tokenizer.EncodeAsIds(eng_corpus[idx])  # 영어 문장 토큰화\n","\n","    if len(src_tok) <= 50 and len(tgt_tok) <= 50:  # 토큰 길이가 50 이하인 경우만 선택\n","        src_corpus.append(src_tok)\n","        tgt_corpus.append(tgt_tok)\n","\n","# 패딩처리를 완료하여 학습용 데이터를 완성합니다.\n","enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n","dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"]},{"cell_type":"code","source":["# 원하는 인덱스의 데이터에 대해서만 수행\n","indices_to_check = [1, 10000, 30000, 50000]\n","\n","for index in indices_to_check:\n","    src_sequence = enc_train[index - 1]  # 인덱스는 1부터 시작하므로 -1\n","    tgt_sequence = dec_train[index - 1]  # 인덱스는 1부터 시작하므로 -1\n","\n","    # 패딩을 제외하고 원래 문장으로 변환\n","    src_tokens = [ko_tokenizer.IdToPiece(int(token)) for token in src_sequence if token != 0]\n","    tgt_tokens = [en_tokenizer.IdToPiece(int(token)) for token in tgt_sequence if token != 0]\n","\n","    print(f\"Sample {index}:\")\n","    print(\"소스 시퀀스 (패딩 포함):\", src_sequence)\n","    print(\"타겟 시퀀스 (패딩 포함):\", tgt_sequence)\n","    print(\"소스 문장 (패딩 제외):\", \" \".join(src_tokens))\n","    print(\"타겟 문장 (패딩 제외):\", \" \".join(tgt_tokens))\n","    print()"],"metadata":{"id":"YLCl1u3IODHr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k1_AI_rVoC36"},"source":["## \b트랜스포머 설계"]},{"cell_type":"code","source":["import numpy as np\n","\n","# 포지셔널 인코딩\n","def positional_encoding(pos, d_model):\n","    def cal_angle(position, i):\n","        return position / np.power(10000, int(i) / d_model)\n","\n","    def get_posi_angle_vec(position):\n","        return [cal_angle(position, i) for i in range(d_model)]\n","\n","    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n","    return sinusoid_table"],"metadata":{"id":"NEgQdZqxF2Ga"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n","        self.W_k = tf.keras.layers.Dense(d_model)\n","        self.W_v = tf.keras.layers.Dense(d_model)\n","\n","        self.linear = tf.keras.layers.Dense(d_model)\n","\n","    def scaled_dot_product_attention(self, Q, K, V, mask):\n","        print(f\"Q shape after split: {Q.shape}\")\n","        print(f\"K shape after split: {K.shape}\")\n","        print(f\"V shape after split: {V.shape}\")\n","        d_k = tf.cast(K.shape[-1], tf.float32)\n","        QK = tf.matmul(Q, K, transpose_b=True)\n","\n","        scaled_qk = QK / tf.math.sqrt(d_k)\n","        print(f\"Scaled QK shape before adding mask: {scaled_qk.shape}\")\n","\n","        if mask is not None:\n","          print(f\"Mask shape: {mask.shape}\")\n","          scaled_qk += (mask * -1e9)\n","\n","        \"\"\"\n","        1. Attention Weights 값 구하기 -> attentions\n","        2. Attention 값을 V에 곱하기 -> out\n","        \"\"\"\n","        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n","        out = tf.matmul(attentions, V)\n","\n","        return out, attentions\n","\n","\n","    def split_heads(self, x):\n","        \"\"\"\n","        Embedding을 Head의 수로 분할하는 함수\n","\n","        x: [ batch x length x emb ]\n","        return: [ batch x heads x length x self.depth ]\n","        \"\"\"\n","\n","        batch_size = x.shape[0]\n","        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n","\n","        return split_x\n","\n","    def combine_heads(self, x):\n","        \"\"\"\n","        분할된 Embedding을 하나로 결합하는 함수\n","\n","        x: [ batch x heads x length x self.depth ]\n","        return: [ batch x length x emb ]\n","        \"\"\"\n","\n","        batch_size = x.shape[0]\n","        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n","        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n","\n","\n","        return combined_x\n","\n","\n","    def call(self, Q, K, V, mask):\n","        \"\"\"\n","        아래 순서에 따라 소스를 작성하세요.\n","\n","        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n","        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n","        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n","                 -> out, attention_weights\n","        Step 4: Combine Heads(out) -> out\n","        Step 5: Linear_out(out) -> out\n","\n","        \"\"\"\n","        print(f\"Q shape: {Q.shape}\")\n","        print(f\"K shape: {K.shape}\")\n","        print(f\"V shape: {V.shape}\")\n","        WQ = self.W_q(Q)\n","        WK = self.W_k(K)\n","        WV = self.W_v(V)\n","        WQ_splits = self.split_heads(WQ)\n","        WK_splits = self.split_heads(WK)\n","        WV_splits = self.split_heads(WV)\n","\n","        out, attention_weights = self.scaled_dot_product_attention(\n","            WQ_splits, WK_splits, WV_splits, mask)\n","\n","        out = self.combine_heads(out)\n","        out = self.linear(out)\n","\n","        return out, attention_weights"],"metadata":{"id":"YtqjfPbcF4y3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Position-wise FFN\n","\n","class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n","    def __init__(self, d_model, d_ff):\n","        super(PoswiseFeedForwardNet, self).__init__()\n","        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n","        self.w_2 = tf.keras.layers.Dense(d_model)\n","\n","    def call(self, x):\n","        out = self.w_1(x)\n","        out = self.w_2(out)\n","\n","        return out"],"metadata":{"id":"EVSAKV2MPBUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dJvA6kAl2V2"},"outputs":[],"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, n_heads, d_ff, dropout):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n","        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n","\n","        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout = tf.keras.layers.Dropout(dropout)\n","\n","    def call(self, x, mask):\n","\n","        # 입력 차원 출력\n","        print(f\"Encoder Layer input shape: {x.shape}\")\n","\n","        \"\"\"\n","        Multi-Head Attention\n","        \"\"\"\n","        residual = x\n","        out = self.norm_1(x)\n","        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n","        out = self.dropout(out)\n","        out += residual\n","\n","        \"\"\"\n","        Position-Wise Feed Forward Network\n","        \"\"\"\n","        residual = out\n","        out = self.norm_2(out)\n","        out = self.ffn(out)\n","        out = self.dropout(out)\n","        out += residual\n","\n","        # 출력 차원 출력\n","        print(f\"Encoder Layer output shape: {out.shape}\")\n","\n","        return out, enc_attn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0R4ugOCIrL3j"},"outputs":[],"source":["class DecoderLayer(tf.keras.Model):\n","    def __init__(self, d_model, n_heads, d_ff, dropout):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.dec_self_attn = MultiHeadAttention(d_model, n_heads)\n","        self.enc_dec_attn = MultiHeadAttention(d_model, n_heads)\n","\n","        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n","\n","        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout = tf.keras.layers.Dropout(dropout)\n","\n","    def call(self, x, enc_out, causality_mask, padding_mask):\n","\n","        \"\"\"\n","        Masked Multi-Head Attention\n","        \"\"\"\n","\n","        residual = x\n","        out = self.norm_1(x)\n","        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n","        out = self.dropout(out)\n","        out += residual\n","\n","        \"\"\"\n","        Multi-Head Attention\n","        \"\"\"\n","\n","        residual = out\n","        out = self.norm_2(out)\n","        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n","        out + self.dropout(out)\n","        out += residual\n","\n","        \"\"\"\n","        Position-Wise Feed Forward Network\n","        \"\"\"\n","\n","        out = residual\n","        out = self.norm_3(out)\n","        out = self.ffn(out)\n","        out = self.dropout(out)\n","        out += residual\n","\n","        return out, dec_attn, dec_enc_attn"]},{"cell_type":"code","source":["# 인코더와 디코더 클래스\n","class Encoder(tf.keras.Model):\n","    def __init__(self,\n","                 n_layers,\n","                 d_model,\n","                 n_heads,\n","                 d_ff,\n","                 dropout):\n","        super(Encoder, self).__init__()\n","        self.n_layers = n_layers\n","        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout)\n","                        for _ in range(n_layers)]\n","\n","    def call(self, x, mask):\n","        print(f\"Encoder Layer input shape before embedding: {x.shape}\")\n","        out = x\n","\n","        enc_attns = list()\n","        for i in range(self.n_layers):\n","            out, enc_attn = self.enc_layers[i](out, mask)\n","            enc_attns.append(enc_attn)\n","\n","        return out, enc_attns\n","\n","class Decoder(tf.keras.Model):\n","    def __init__(self,\n","                 n_layers,\n","                 d_model,\n","                 n_heads,\n","                 d_ff,\n","                 dropout):\n","        super(Decoder, self).__init__()\n","        self.n_layers = n_layers\n","        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout)\n","                            for _ in range(n_layers)]\n","\n","\n","    def call(self, x, enc_out, causality_mask, padding_mask):\n","        print(f\"Causality mask shape: {causality_mask.shape}\")\n","        print(f\"Padding mask shape: {padding_mask.shape}\")\n","        out = x\n","\n","        dec_attns = list()\n","        dec_enc_attns = list()\n","        for i in range(self.n_layers):\n","            out, dec_attn, dec_enc_attn = \\\n","            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n","\n","            dec_attns.append(dec_attn)\n","            dec_enc_attns.append(dec_enc_attn)\n","\n","        return out, dec_attns, dec_enc_attns"],"metadata":{"id":"Mj_kw8jjS54i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(tf.keras.Model):\n","    def __init__(self,\n","                 n_layers,\n","                 d_model,\n","                 n_heads,\n","                 d_ff,\n","                 src_vocab_size,\n","                 tgt_vocab_size,\n","                 pos_len,\n","                 dropout=0.2,\n","                 shared=True):\n","        super(Transformer, self).__init__()\n","        self.d_model = tf.cast(d_model, tf.float32)\n","\n","        \"\"\"\n","        1. Embedding Layer 정의\n","        2. Positional Encoding 정의\n","        3. Encoder / Decoder 정의\n","        4. Output Linear 정의\n","        5. Shared Weights\n","        6. Dropout 정의\n","        \"\"\"\n","        self.enc_embedding = tf.keras.layers.Embedding(src_vocab_size, d_model)\n","        self.dec_embedding = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n","\n","        self.positional_encoding = positional_encoding(pos_len, d_model)\n","\n","        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n","        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n","\n","        self.output_fc = tf.keras.layers.Dense(tgt_vocab_size)\n","\n","        self.shared_weights = shared\n","        self.dropout = tf.keras.layers.Dropout(dropout)\n","\n","\n","    def embedding(self, emb, x):\n","        \"\"\"\n","        입력된 정수 배열을 Embedding + Pos Encoding\n","        + Shared일 경우 Scaling 작업 포함\n","\n","        x: [ batch x length ]\n","        return: [ batch x length x emb ]\n","        \"\"\"\n","\n","        seq_len = x.shape[1]\n","        out = emb(x)\n","\n","        if self.shared_weights: out *= tf.math.sqrt(self.d_model)\n","\n","        out += self.positional_encoding[np.newaxis, ...][:, :seq_len, :]\n","        out = self.dropout(out)\n","\n","        return out\n","\n","\n","    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n","        \"\"\"\n","        아래 순서에 따라 소스를 작성하세요.\n","\n","        Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n","        Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n","        Step 3: Decoder(dec_in, enc_out, mask)\n","                -> dec_out, dec_attns, dec_enc_attns\n","        Step 4: Out Linear(dec_out) -> logits\n","        \"\"\"\n","\n","        enc_in = self.embedding(self.enc_embedding, enc_in)\n","        dec_in = self.embedding(self.dec_embedding, dec_in)\n","\n","        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n","        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n","\n","        logits = self.output_fc(dec_out)\n","\n","        return logits, enc_attns, dec_attns, dec_enc_attns\n"],"metadata":{"id":"0e2CtJ0XTCpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Masking\n","\n","def generate_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]\n","\n","def generate_causality_mask(src_len, tgt_len):\n","    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n","    return tf.cast(mask, tf.float32)\n","\n","def generate_masks(src, tgt):\n","    enc_mask = generate_padding_mask(src)\n","    dec_mask = generate_padding_mask(tgt)\n","\n","    # 차원 출력\n","    print(f\"Encoder mask shape: {enc_mask.shape}\")\n","    print(f\"Decoder mask shape: {dec_mask.shape}\")\n","\n","    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n","    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n","\n","    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n","    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n","\n","    return enc_mask, dec_enc_mask, dec_mask"],"metadata":{"id":"5mTLweDJWEPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 마스크 작동방식 확인\n","import matplotlib.pyplot as plt\n","\n","batch, length = 16, 20\n","src_padding = 5\n","tgt_padding = 15\n","\n","src_pad = tf.zeros(shape=(batch, src_padding))\n","tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n","\n","sample_data = tf.ones(shape=(batch, length))\n","\n","sample_src = tf.concat([sample_data, src_pad], axis=-1)\n","sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n","\n","enc_mask, dec_enc_mask, dec_mask = \\\n","generate_masks(sample_src, sample_tgt)\n","\n","fig = plt.figure(figsize=(7, 7))\n","\n","ax1 = fig.add_subplot(131)\n","ax2 = fig.add_subplot(132)\n","ax3 = fig.add_subplot(133)\n","\n","ax1.set_title('1) Encoder Mask')\n","ax2.set_title('2) Encoder-Decoder Mask')\n","ax3.set_title('3) Decoder Mask')\n","\n","ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n","ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n","ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n","\n","plt.show()"],"metadata":{"id":"AP3QaV1uXDDb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import LearningRateScheduler\n","# 학습률 조정자\n","class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(LearningRateScheduler, self).__init__()\n","        self.d_model = tf.cast(d_model, tf.float32)\n","        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n","\n","    def __call__(self, step):\n","        step = tf.cast(step, tf.float32)  # step을 float32로 변환\n","        arg1 = tf.math.pow(step, -0.5)\n","        arg2 = step * tf.math.pow(self.warmup_steps, -1.5)\n","\n","        return tf.math.pow(self.d_model, -0.5) * tf.math.minimum(arg1, arg2)\n","\n","learning_rate = LearningRateScheduler(512)\n","optimizer = tf.keras.optimizers.Adam(learning_rate,\n","                                     beta_1=0.9,\n","                                     beta_2=0.98,\n","                                     epsilon=1e-9)"],"metadata":{"id":"vyDUc-leXV_f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_A1ayTXxzb5"},"source":["## 훈련"]},{"cell_type":"code","source":["# 트랜스포머 선언\n","\n","# Hyperparameters\n","N_LAYERS = 2\n","D_MODEL = 512\n","N_HEADS = 8\n","D_FF = 2048\n","POS_LEN = 200  # 이 값은 데이터셋에 따라 조정될 수 있습니다.\n","DROPOUT = 0.1\n","\n","# Transformer 인스턴스 생성\n","transformer = Transformer(\n","    n_layers=N_LAYERS,\n","    d_model=D_MODEL,\n","    n_heads=N_HEADS,\n","    d_ff=D_FF,\n","    src_vocab_size=SRC_VOCAB_SIZE,\n","    tgt_vocab_size=TGT_VOCAB_SIZE,\n","    pos_len=POS_LEN,\n","    dropout=DROPOUT\n",")\n"],"metadata":{"id":"y_eK0kjrZ39f"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MftGuotY0M5a"},"outputs":[],"source":["# 손실함수 정의\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qm5vs83opH7O"},"outputs":[],"source":["# train_step 함수 완성\n","@tf.function()\n","def train_step(src, tgt, model, optimizer):\n","    try:\n","        gold = tgt[:, 1:]\n","\n","        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n","\n","        with tf.GradientTape() as tape:\n","            predictions, enc_attns, dec_attns, dec_enc_attns = \\\n","            model(src, tgt[:, :-1], enc_mask, dec_enc_mask, dec_mask)\n","            loss = loss_function(gold, predictions)\n","\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","        return loss, enc_attns, dec_attns, dec_enc_attns\n","    except Exception as e:\n","        print(\"An error occurred in train_step:\")\n","        print(str(e))\n","        # 여기에 필요한 경우 추가적인 오류 로깅 또는 처리를 추가할 수 있습니다.\n"]},{"cell_type":"code","source":["# Attention 시각화 함수\n","\n","def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n","    def draw(data, ax, x=\"auto\", y=\"auto\"):\n","        import seaborn\n","        seaborn.heatmap(data,\n","                        square=True,\n","                        vmin=0.0, vmax=1.0,\n","                        cbar=False, ax=ax,\n","                        xticklabels=x,\n","                        yticklabels=y)\n","\n","    for layer in range(0, 2, 1):\n","        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n","        print(\"Encoder Layer\", layer + 1)\n","        for h in range(4):\n","            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n","        plt.show()\n","\n","    for layer in range(0, 2, 1):\n","        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n","        print(\"Decoder Self Layer\", layer+1)\n","        for h in range(4):\n","            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n","        plt.show()\n","\n","        print(\"Decoder Src Layer\", layer+1)\n","        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n","        for h in range(4):\n","            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n","        plt.show()\n","\n","# 번역 생성 함수\n","\n","def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n","    sentence = preprocess_sentence(sentence)\n","\n","    pieces = src_tokenizer.encode_as_pieces(sentence)\n","    tokens = src_tokenizer.encode_as_ids(sentence)\n","\n","    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n","                                                           maxlen=enc_train.shape[-1],\n","                                                           padding='post')\n","\n","    ids = []\n","    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n","    for i in range(dec_train.shape[-1]):\n","        enc_padding_mask, combined_mask, dec_padding_mask = \\\n","        generate_masks(_input, output)\n","\n","        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n","        model(_input,\n","              output,\n","              enc_padding_mask,\n","              combined_mask,\n","              dec_padding_mask)\n","\n","        predicted_id = \\\n","        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n","\n","        if tgt_tokenizer.eos_id() == predicted_id:\n","            result = tgt_tokenizer.decode_ids(ids)\n","            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n","\n","        ids.append(predicted_id)\n","        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n","\n","    result = tgt_tokenizer.decode_ids(ids)\n","\n","    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n","\n","# 번역 생성 및 Attention 시각화 결합\n","\n","def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n","    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n","    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n","\n","    print('Input: %s' % (sentence))\n","    print('Predicted translation: {}'.format(result))\n","\n","    if plot_attention:\n","        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"],"metadata":{"id":"GoR8vBXNgBWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DEvaYi04pH7O"},"outputs":[],"source":["# 학습\n","\n","import random\n","from tqdm import tqdm_notebook\n","\n","BATCH_SIZE = 64\n","EPOCHS = 20\n","\n","examples = [\n","            \"오바마는 대통령이다.\",\n","            \"시민들은 도시 속에 산다.\",\n","            \"커피는 필요 없다.\",\n","            \"일곱 명의 사망자가 발생했다.\"\n","]\n","\n","for epoch in range(EPOCHS):\n","    total_loss = 0\n","\n","    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n","    random.shuffle(idx_list)\n","    t = tqdm(idx_list)\n","\n","    for (batch, idx) in enumerate(t):\n","        batch_enc_train = enc_train[idx:min(idx + BATCH_SIZE, enc_train.shape[0])]\n","        batch_dec_train = dec_train[idx:min(idx + BATCH_SIZE, dec_train.shape[0])]\n","\n","        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n","        train_step(batch_enc_train, batch_dec_train, transformer, optimizer)\n","\n","        total_loss += batch_loss\n","\n","        t.set_description_str('Epoch %2d' % (epoch + 1))\n","        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n","\n","    for example in examples:\n","        translate(example, transformer, ko_tokenizer, en_tokenizer)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"VurmOIWCe9AN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N9l_B4n9pH7P"},"source":["## 테스트"]},{"cell_type":"markdown","metadata":{"id":"yyytXWcppH7P"},"source":["# 회고\n","\n","모델 작성을 마치고, 텐서 형태가 맞지 않는다는 에러가 발생해서 GPT의 도움으로 디버깅을 시도했다. 차원 숫자를 모두 출력해서 확인하는 방식으로 코드 수정을 제안받아서 계속 몇 차례 출력문을 집어넣다가, 'enc_in'이 들어가야 할 자리에 'emc_in'이 들어가 있다는 것을 발견했다.\n","\n","학습을 위해서 코드를 직접 따라치는 과정에서 오타가 생긴 모양이었다.\n","\n","GPT가 없었다면 아마 발견도 못했을 것 같은데... 학습을 위해서 직접 코딩을 할 때에는 타이핑 후에 무결한 코드를 붙여넣는 방식으로 마무리를 하는 게 좋겠다는 생각이 든다."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"15Qi7PtG8dmPCX8sdX1LGzuyZ_ey48ILu","timestamp":1700096413854}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}