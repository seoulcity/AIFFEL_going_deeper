{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1CD5n4tK69Bz28TyLnCJXSyxM4J9nUmN5","timestamp":1699600988958}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 프로젝트\n","## 개요\n","영화 시놉시스 코퍼스를 이용해, 영화 장르에 따른 상업-예술영화의 표현 편향성을 측정한다.\n","\n","## 순서\n","- STEP 1. 형태소 분석기를 이용하여 품사가 명사인 경우 해당 단어를 추출하기\n","- STEP 2. 추출된 결과로 embedding model 만들기\n","- STEP 3. target, attribute 단어 셋 만들기\n","- STEP 4. WEAT score 계산과 시각화\n","\n","## 달성 목표\n","### 1. 주어진 영화 코퍼스를 바탕으로 워드임베딩 모델 구축\n","- 워드임베딩의 most_similar() 메소드 결과가 의미상 바르게 나와야 한다.\n","\n","### 2. 영화 구분, 장르별로 target, attribute에 대한 대표성있는 단어 셋을 생성\n","-\t타당한 방법론을 통해 중복이 잘 제거되고 개념축을 의미적으로 잘 대표하는 단어 셋 구축\n","\n","### 3. WEAT score 계산 및 시각화\n","- 전체 영화 장르별로 예술/일반 영화에 대한 편향성 WEAT score가 상식에 부합하는 수치로 얻어졌으며 이를 잘 시각화"],"metadata":{"id":"hzl4LFWFJBlf"}},{"cell_type":"markdown","source":[],"metadata":{"id":"NInmYynIkx-L"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"p8TCZxd7MImx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699618339578,"user_tz":-540,"elapsed":24456,"user":{"displayName":"Junghyun Joseph Kim","userId":"06138359960746591941"}},"outputId":"e27c2c28-86e9-4ec3-db66-3a25726472a6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install konlpy\n","!pip install gensim"],"metadata":{"id":"ae4DtrGAI6hO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699618351109,"user_tz":-540,"elapsed":11534,"user":{"displayName":"Junghyun Joseph Kim","userId":"06138359960746591941"}},"outputId":"d9acd795-4343-4456-fc66-bdfde3271d0e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import os\n","import numpy as np\n","from functools import lru_cache\n","from gensim.models import Word2Vec\n","from concurrent.futures import ThreadPoolExecutor\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","class TextProcessor:\n","    def __init__(self, data_path):\n","        self.data_path = data_path\n","        self.okt = Okt()\n","        self.vectorizer = TfidfVectorizer()\n","\n","    @lru_cache(maxsize=None)\n","    def tokenize(self, file_name):\n","        print(f\"[{file_name} 파일 토큰화] 🔄\")\n","        result = []\n","        with open(os.path.join(self.data_path, file_name), 'r', encoding='utf-8') as file:\n","            for line in file:\n","                tokenized_line = self.okt.pos(line, stem=True, norm=True)\n","                result.extend([word[0] for word in tokenized_line if word[1] in [\"Noun\"]])\n","        print(f\"{file_name} [토큰화 완료] ✅: 총 {len(result)}개의 토큰\")\n","        return result\n","\n","    def build_word_set(self, file_name, texts, top_n=100):\n","        print(f\"[{file_name}에 대한 대표 단어 셋 구축] 🔄\")\n","        self.vectorizer.fit(texts)\n","        indices = np.argsort(self.vectorizer.idf_)[::-1]\n","        features = self.vectorizer.get_feature_names_out()\n","        top_features = [features[i] for i in indices[:top_n]]\n","        print(f\"[{file_name}에 대한 단어 셋 구축 완료] ✅: 상위 {top_n}개 단어 선택\")\n","        return top_features\n","\n","class WEATCalculator:\n","    def __init__(self, model):\n","        self.model = model\n","\n","    @staticmethod\n","    def cos_sim(i, j):\n","        return np.dot(i, j) / (np.linalg.norm(i) * np.linalg.norm(j))\n","\n","    def s(self, w, A, B):\n","        c_a = np.mean([self.cos_sim(w, a) for a in A])\n","        c_b = np.mean([self.cos_sim(w, b) for b in B])\n","        return c_a - c_b\n","\n","    def weat_score(self, X, Y, A, B):\n","        s_X = np.mean([self.s(x, A, B) for x in X])\n","        s_Y = np.mean([self.s(y, A, B) for y in Y])\n","        std_dev = np.std(np.concatenate([s_X, s_Y], axis=None))\n","        score = (s_X - s_Y) / std_dev\n","        return score"],"metadata":{"id":"8toOQ_8qHjtA","executionInfo":{"status":"ok","timestamp":1699618352742,"user_tz":-540,"elapsed":1636,"user":{"displayName":"Junghyun Joseph Kim","userId":"06138359960746591941"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 데이터 경로 및 장르별 파일 목록 설정\n","data_path = '/content/drive/MyDrive/Colab Notebooks/AIFFEL_Research_6th/going_deeper/07-09_Embedded_bias/synopsis'\n","genre_files = [\n","    'synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt',\n","    'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt',\n","    'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt',\n","    'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt',\n","    'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt'\n","]"],"metadata":{"id":"8lEXxwDFCNtz","executionInfo":{"status":"ok","timestamp":1699618352742,"user_tz":-540,"elapsed":3,"user":{"displayName":"Junghyun Joseph Kim","userId":"06138359960746591941"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["text_processor = TextProcessor(data_path)\n","genre_word_sets = {}\n","\n","with ThreadPoolExecutor() as executor:\n","    futures = {genre: executor.submit(text_processor.tokenize, genre) for genre in genre_files}\n","    for genre, future in futures.items():\n","        tokenized_text = future.result()\n","        word_set = text_processor.build_word_set(genre, [' '.join(tokenized_text)], 100)\n","        genre_word_sets[genre] = word_set\n","\n"],"metadata":{"id":"V--V38k2CNo4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699618803822,"user_tz":-540,"elapsed":451082,"user":{"displayName":"Junghyun Joseph Kim","userId":"06138359960746591941"}},"outputId":"cbcf545f-cf53-499b-931a-0c85bdee1e48"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[synopsis_SF.txt 파일 토큰화] 🔄\n","[synopsis_family.txt 파일 토큰화] 🔄\n","[synopsis_show.txt 파일 토큰화] 🔄\n","[synopsis_horror.txt 파일 토큰화] 🔄\n","[synopsis_etc.txt 파일 토큰화] 🔄\n","[synopsis_documentary.txt 파일 토큰화] 🔄\n","synopsis_family.txt [토큰화 완료] ✅: 총 8006개의 토큰\n","[synopsis_drama.txt 파일 토큰화] 🔄\n","synopsis_show.txt [토큰화 완료] ✅: 총 7984개의 토큰\n","[synopsis_romance.txt 파일 토큰화] 🔄\n","synopsis_SF.txt [토큰화 완료] ✅: 총 21712개의 토큰\n","[synopsis_musical.txt 파일 토큰화] 🔄\n","[synopsis_SF.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_SF.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_family.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_family.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_show.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_show.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","synopsis_musical.txt [토큰화 완료] ✅: 총 3442개의 토큰\n","[synopsis_mystery.txt 파일 토큰화] 🔄\n","synopsis_mystery.txt [토큰화 완료] ✅: 총 15726개의 토큰\n","[synopsis_crime.txt 파일 토큰화] 🔄\n","synopsis_etc.txt [토큰화 완료] ✅: 총 49025개의 토큰\n","[synopsis_historical.txt 파일 토큰화] 🔄\n","synopsis_historical.txt [토큰화 완료] ✅: 총 3620개의 토큰\n","[synopsis_western.txt 파일 토큰화] 🔄\n","synopsis_western.txt [토큰화 완료] ✅: 총 925개의 토큰\n","[synopsis_adult.txt 파일 토큰화] 🔄\n","synopsis_horror.txt [토큰화 완료] ✅: 총 57802개의 토큰\n","[synopsis_thriller.txt 파일 토큰화] 🔄\n","[synopsis_horror.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_horror.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_etc.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_etc.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","synopsis_crime.txt [토큰화 완료] ✅: 총 29567개의 토큰\n","[synopsis_animation.txt 파일 토큰화] 🔄\n","synopsis_thriller.txt [토큰화 완료] ✅: 총 43043개의 토큰\n","[synopsis_action.txt 파일 토큰화] 🔄\n","synopsis_adult.txt [토큰화 완료] ✅: 총 45227개의 토큰\n","[synopsis_adventure.txt 파일 토큰화] 🔄\n","synopsis_romance.txt [토큰화 완료] ✅: 총 102083개의 토큰\n","[synopsis_war.txt 파일 토큰화] 🔄\n","synopsis_adventure.txt [토큰화 완료] ✅: 총 9687개의 토큰\n","[synopsis_comedy.txt 파일 토큰화] 🔄\n","synopsis_war.txt [토큰화 완료] ✅: 총 8462개의 토큰\n","[synopsis_fantasy.txt 파일 토큰화] 🔄\n","synopsis_fantasy.txt [토큰화 완료] ✅: 총 11994개의 토큰\n","synopsis_documentary.txt [토큰화 완료] ✅: 총 156822개의 토큰\n","[synopsis_documentary.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_documentary.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","synopsis_animation.txt [토큰화 완료] ✅: 총 121876개의 토큰\n","synopsis_action.txt [토큰화 완료] ✅: 총 115561개의 토큰\n","synopsis_comedy.txt [토큰화 완료] ✅: 총 107432개의 토큰\n","synopsis_drama.txt [토큰화 완료] ✅: 총 377566개의 토큰\n","[synopsis_drama.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_drama.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_romance.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_romance.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_musical.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_musical.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_mystery.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_mystery.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_crime.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_crime.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_historical.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_historical.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_western.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_western.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_adult.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_adult.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_thriller.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_thriller.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_animation.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_animation.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_action.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_action.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_adventure.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_adventure.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_war.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_war.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_comedy.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_comedy.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n","[synopsis_fantasy.txt에 대한 대표 단어 셋 구축] 🔄\n","[synopsis_fantasy.txt에 대한 단어 셋 구축 완료] ✅: 상위 100개 단어 선택\n"]}]},{"cell_type":"code","source":["tokenized_art = text_processor.tokenize('synopsis_art.txt')\n","tokenized_gen = text_processor.tokenize('synopsis_gen.txt')\n","\n"],"metadata":{"id":"lROOuaHlCZEy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699619287761,"user_tz":-540,"elapsed":483952,"user":{"displayName":"Junghyun Joseph Kim","userId":"06138359960746591941"}},"outputId":"75d63ca9-2fcb-4017-976c-8ff0fefc6e83"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[synopsis_art.txt 파일 토큰화] 🔄\n","synopsis_art.txt [토큰화 완료] ✅: 총 208471개의 토큰\n","[synopsis_gen.txt 파일 토큰화] 🔄\n","synopsis_gen.txt [토큰화 완료] ✅: 총 1008358개의 토큰\n"]}]},{"cell_type":"code","source":["model = Word2Vec([tokenized_art + tokenized_gen], vector_size=100, window=5, min_count=3, sg=0)\n","weat_calculator = WEATCalculator(model.wv)\n","\n","X = np.array([model.wv[word] for word in tokenized_art if word in model.wv])\n","Y = np.array([model.wv[word] for word in tokenized_gen if word in model.wv])\n","\n"],"metadata":{"id":"woZzuFq7GysJ","executionInfo":{"status":"ok","timestamp":1699619290920,"user_tz":-540,"elapsed":3170,"user":{"displayName":"Junghyun Joseph Kim","userId":"06138359960746591941"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["matrix = np.zeros((len(genre_files), len(genre_files)))\n","for i, genre_A in enumerate(genre_word_sets.values()):\n","    for j, genre_B in enumerate(genre_word_sets.values()):\n","        if i >= j:\n","            continue\n","        A = np.array([model.wv[word] for word in genre_A if word in model.wv])\n","        B = np.array([model.wv[word] for word in genre_B if word in model.wv])\n","        score = weat_calculator.weat_score(X, Y, A, B)\n","        matrix[i][j] = score\n","        matrix[j][i] = -score\n","\n"],"metadata":{"id":"_ue2ZTOgCbep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["genre_labels = [os.path.splitext(os.path.basename(genre))[0] for genre in genre_files]\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(matrix, xticklabels=genre_labels, yticklabels=genre_labels, cmap='coolwarm', annot=True)\n","plt.title('Genre Bias Heatmap')\n","plt.xlabel('Genre A')\n","plt.ylabel('Genre B')\n","plt.show()"],"metadata":{"id":"U9AEYySCCeNP"},"execution_count":null,"outputs":[]}]}